{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"credit_card_fraud_detection.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["T8BvSgLzQCdZ","gSBDbmb9QZ8c","BeCU0zNrQzB7","63piDKReSyz7","1m7rcSLcTHyK","ZpjpF4HNTW_W","Dfnh8OcwTZDt","7OuQmAEMTb51","NlrBY5FpT1Hb","IwkZzFEdUBlB","vIhEONijUG8b","wMTdv-i6UR0W","BfnLuoxEUVaF","ZqjVkD3TUo_6","4hJ3stYKUzXa","XnDC0pWoU881","5k67G_3xeSnv","beiyDdIWeiH8","WR97sWxpgawk","8DxGjRS-grKm","8Rzk06xWgtQ4","APQojBRDg0Vy","yuSHEkJpg8MB","oFgxn2QrhEH3","nq-sYzNb-OwA","36RqsHC1-W2-","V6ykzrVg-XYF"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"FcuWdnjBP2Ua","colab_type":"text"},"cell_type":"markdown","source":["# Comparative Study of Supervised Learning Methods for Credit Card Fraud Detection"]},{"metadata":{"id":"T8BvSgLzQCdZ","colab_type":"text"},"cell_type":"markdown","source":["### 0) Set-up the notebook"]},{"metadata":{"colab_type":"code","outputId":"ae67c7c5-dd37-4a81-c723-ab5c86cc4340","executionInfo":{"status":"ok","timestamp":1541818854584,"user_tz":-480,"elapsed":12193,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}},"id":"4jwEP6R91IsB","colab":{"base_uri":"https://localhost:8080/","height":292}},"cell_type":"code","source":["# hide any warnings\n","import sys\n","import warnings\n","if not sys.warnoptions:\n","    warnings.simplefilter(\"ignore\")\n","\n","# install module that contains sampling methods\n","!pip install imblearn"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting imblearn\n","  Downloading https://files.pythonhosted.org/packages/81/a7/4179e6ebfd654bd0eac0b9c06125b8b4c96a9d0a8ff9e9507eb2a26d2d7e/imblearn-0.0-py2.py3-none-any.whl\n","Collecting imbalanced-learn (from imblearn)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/4c/7557e1c2e791bd43878f8c82065bddc5798252084f26ef44527c02262af1/imbalanced_learn-0.4.3-py3-none-any.whl (166kB)\n","\u001b[K    100% |████████████████████████████████| 174kB 7.0MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (1.14.6)\n","Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (1.1.0)\n","Collecting scikit-learn>=0.20 (from imbalanced-learn->imblearn)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/b2/05be9b6da9ae4a4c54f537be22e95833f722742a02b1e355fdc09363877c/scikit_learn-0.20.0-cp36-cp36m-manylinux1_x86_64.whl (5.3MB)\n","\u001b[K    100% |████████████████████████████████| 5.3MB 6.7MB/s \n","\u001b[?25hInstalling collected packages: scikit-learn, imbalanced-learn, imblearn\n","  Found existing installation: scikit-learn 0.19.2\n","    Uninstalling scikit-learn-0.19.2:\n","      Successfully uninstalled scikit-learn-0.19.2\n","Successfully installed imbalanced-learn-0.4.3 imblearn-0.0 scikit-learn-0.20.0\n"],"name":"stdout"}]},{"metadata":{"id":"Mb3V1lG01Bzm","colab_type":"code","colab":{}},"cell_type":"code","source":["# import libraries\n","import io\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, fbeta_score"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gSBDbmb9QZ8c","colab_type":"text"},"cell_type":"markdown","source":["### 1) Load dataset"]},{"metadata":{"colab_type":"code","id":"U9uTFkll1WEE","colab":{}},"cell_type":"code","source":["# load dataset to colab notebook\n","from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UaND0-H91J25","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":253},"outputId":"1c8d42bc-0ef8-4f47-88f9-ada2d728fecc","executionInfo":{"status":"ok","timestamp":1541818919791,"user_tz":-480,"elapsed":5769,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# observe that the dataset is in chronological order\n","df = pd.read_csv('drive/My Drive/CS3244 Credit Card Fraud Detection/Machine Learning/credit_card.csv')\n","df.head()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Time</th>\n","      <th>V1</th>\n","      <th>V2</th>\n","      <th>V3</th>\n","      <th>V4</th>\n","      <th>V5</th>\n","      <th>V6</th>\n","      <th>V7</th>\n","      <th>V8</th>\n","      <th>V9</th>\n","      <th>...</th>\n","      <th>V21</th>\n","      <th>V22</th>\n","      <th>V23</th>\n","      <th>V24</th>\n","      <th>V25</th>\n","      <th>V26</th>\n","      <th>V27</th>\n","      <th>V28</th>\n","      <th>Amount</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>-1.359807</td>\n","      <td>-0.072781</td>\n","      <td>2.536347</td>\n","      <td>1.378155</td>\n","      <td>-0.338321</td>\n","      <td>0.462388</td>\n","      <td>0.239599</td>\n","      <td>0.098698</td>\n","      <td>0.363787</td>\n","      <td>...</td>\n","      <td>-0.018307</td>\n","      <td>0.277838</td>\n","      <td>-0.110474</td>\n","      <td>0.066928</td>\n","      <td>0.128539</td>\n","      <td>-0.189115</td>\n","      <td>0.133558</td>\n","      <td>-0.021053</td>\n","      <td>149.62</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>1.191857</td>\n","      <td>0.266151</td>\n","      <td>0.166480</td>\n","      <td>0.448154</td>\n","      <td>0.060018</td>\n","      <td>-0.082361</td>\n","      <td>-0.078803</td>\n","      <td>0.085102</td>\n","      <td>-0.255425</td>\n","      <td>...</td>\n","      <td>-0.225775</td>\n","      <td>-0.638672</td>\n","      <td>0.101288</td>\n","      <td>-0.339846</td>\n","      <td>0.167170</td>\n","      <td>0.125895</td>\n","      <td>-0.008983</td>\n","      <td>0.014724</td>\n","      <td>2.69</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.0</td>\n","      <td>-1.358354</td>\n","      <td>-1.340163</td>\n","      <td>1.773209</td>\n","      <td>0.379780</td>\n","      <td>-0.503198</td>\n","      <td>1.800499</td>\n","      <td>0.791461</td>\n","      <td>0.247676</td>\n","      <td>-1.514654</td>\n","      <td>...</td>\n","      <td>0.247998</td>\n","      <td>0.771679</td>\n","      <td>0.909412</td>\n","      <td>-0.689281</td>\n","      <td>-0.327642</td>\n","      <td>-0.139097</td>\n","      <td>-0.055353</td>\n","      <td>-0.059752</td>\n","      <td>378.66</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.0</td>\n","      <td>-0.966272</td>\n","      <td>-0.185226</td>\n","      <td>1.792993</td>\n","      <td>-0.863291</td>\n","      <td>-0.010309</td>\n","      <td>1.247203</td>\n","      <td>0.237609</td>\n","      <td>0.377436</td>\n","      <td>-1.387024</td>\n","      <td>...</td>\n","      <td>-0.108300</td>\n","      <td>0.005274</td>\n","      <td>-0.190321</td>\n","      <td>-1.175575</td>\n","      <td>0.647376</td>\n","      <td>-0.221929</td>\n","      <td>0.062723</td>\n","      <td>0.061458</td>\n","      <td>123.50</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2.0</td>\n","      <td>-1.158233</td>\n","      <td>0.877737</td>\n","      <td>1.548718</td>\n","      <td>0.403034</td>\n","      <td>-0.407193</td>\n","      <td>0.095921</td>\n","      <td>0.592941</td>\n","      <td>-0.270533</td>\n","      <td>0.817739</td>\n","      <td>...</td>\n","      <td>-0.009431</td>\n","      <td>0.798278</td>\n","      <td>-0.137458</td>\n","      <td>0.141267</td>\n","      <td>-0.206010</td>\n","      <td>0.502292</td>\n","      <td>0.219422</td>\n","      <td>0.215153</td>\n","      <td>69.99</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 31 columns</p>\n","</div>"],"text/plain":["   Time        V1        V2        V3        V4        V5        V6        V7  \\\n","0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n","1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n","2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n","3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n","4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n","\n","         V8        V9  ...         V21       V22       V23       V24  \\\n","0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n","1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n","2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n","3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n","4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n","\n","        V25       V26       V27       V28  Amount  Class  \n","0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n","1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n","2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n","3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n","4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n","\n","[5 rows x 31 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"metadata":{"id":"nwu8OC7jREIC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":253},"outputId":"d2b44b00-2d7f-4593-a430-8e691b2ac6e2","executionInfo":{"status":"ok","timestamp":1541818920751,"user_tz":-480,"elapsed":800,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["df.tail()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Time</th>\n","      <th>V1</th>\n","      <th>V2</th>\n","      <th>V3</th>\n","      <th>V4</th>\n","      <th>V5</th>\n","      <th>V6</th>\n","      <th>V7</th>\n","      <th>V8</th>\n","      <th>V9</th>\n","      <th>...</th>\n","      <th>V21</th>\n","      <th>V22</th>\n","      <th>V23</th>\n","      <th>V24</th>\n","      <th>V25</th>\n","      <th>V26</th>\n","      <th>V27</th>\n","      <th>V28</th>\n","      <th>Amount</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>284802</th>\n","      <td>172786.0</td>\n","      <td>-11.881118</td>\n","      <td>10.071785</td>\n","      <td>-9.834783</td>\n","      <td>-2.066656</td>\n","      <td>-5.364473</td>\n","      <td>-2.606837</td>\n","      <td>-4.918215</td>\n","      <td>7.305334</td>\n","      <td>1.914428</td>\n","      <td>...</td>\n","      <td>0.213454</td>\n","      <td>0.111864</td>\n","      <td>1.014480</td>\n","      <td>-0.509348</td>\n","      <td>1.436807</td>\n","      <td>0.250034</td>\n","      <td>0.943651</td>\n","      <td>0.823731</td>\n","      <td>0.77</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>284803</th>\n","      <td>172787.0</td>\n","      <td>-0.732789</td>\n","      <td>-0.055080</td>\n","      <td>2.035030</td>\n","      <td>-0.738589</td>\n","      <td>0.868229</td>\n","      <td>1.058415</td>\n","      <td>0.024330</td>\n","      <td>0.294869</td>\n","      <td>0.584800</td>\n","      <td>...</td>\n","      <td>0.214205</td>\n","      <td>0.924384</td>\n","      <td>0.012463</td>\n","      <td>-1.016226</td>\n","      <td>-0.606624</td>\n","      <td>-0.395255</td>\n","      <td>0.068472</td>\n","      <td>-0.053527</td>\n","      <td>24.79</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>284804</th>\n","      <td>172788.0</td>\n","      <td>1.919565</td>\n","      <td>-0.301254</td>\n","      <td>-3.249640</td>\n","      <td>-0.557828</td>\n","      <td>2.630515</td>\n","      <td>3.031260</td>\n","      <td>-0.296827</td>\n","      <td>0.708417</td>\n","      <td>0.432454</td>\n","      <td>...</td>\n","      <td>0.232045</td>\n","      <td>0.578229</td>\n","      <td>-0.037501</td>\n","      <td>0.640134</td>\n","      <td>0.265745</td>\n","      <td>-0.087371</td>\n","      <td>0.004455</td>\n","      <td>-0.026561</td>\n","      <td>67.88</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>284805</th>\n","      <td>172788.0</td>\n","      <td>-0.240440</td>\n","      <td>0.530483</td>\n","      <td>0.702510</td>\n","      <td>0.689799</td>\n","      <td>-0.377961</td>\n","      <td>0.623708</td>\n","      <td>-0.686180</td>\n","      <td>0.679145</td>\n","      <td>0.392087</td>\n","      <td>...</td>\n","      <td>0.265245</td>\n","      <td>0.800049</td>\n","      <td>-0.163298</td>\n","      <td>0.123205</td>\n","      <td>-0.569159</td>\n","      <td>0.546668</td>\n","      <td>0.108821</td>\n","      <td>0.104533</td>\n","      <td>10.00</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>284806</th>\n","      <td>172792.0</td>\n","      <td>-0.533413</td>\n","      <td>-0.189733</td>\n","      <td>0.703337</td>\n","      <td>-0.506271</td>\n","      <td>-0.012546</td>\n","      <td>-0.649617</td>\n","      <td>1.577006</td>\n","      <td>-0.414650</td>\n","      <td>0.486180</td>\n","      <td>...</td>\n","      <td>0.261057</td>\n","      <td>0.643078</td>\n","      <td>0.376777</td>\n","      <td>0.008797</td>\n","      <td>-0.473649</td>\n","      <td>-0.818267</td>\n","      <td>-0.002415</td>\n","      <td>0.013649</td>\n","      <td>217.00</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 31 columns</p>\n","</div>"],"text/plain":["            Time         V1         V2        V3        V4        V5  \\\n","284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n","284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n","284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n","284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n","284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n","\n","              V6        V7        V8        V9  ...         V21       V22  \\\n","284802 -2.606837 -4.918215  7.305334  1.914428  ...    0.213454  0.111864   \n","284803  1.058415  0.024330  0.294869  0.584800  ...    0.214205  0.924384   \n","284804  3.031260 -0.296827  0.708417  0.432454  ...    0.232045  0.578229   \n","284805  0.623708 -0.686180  0.679145  0.392087  ...    0.265245  0.800049   \n","284806 -0.649617  1.577006 -0.414650  0.486180  ...    0.261057  0.643078   \n","\n","             V23       V24       V25       V26       V27       V28  Amount  \\\n","284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n","284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n","284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n","284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n","284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n","\n","        Class  \n","284802      0  \n","284803      0  \n","284804      0  \n","284805      0  \n","284806      0  \n","\n","[5 rows x 31 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"metadata":{"id":"RxB1OJ1pR4M2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"b2fe0d5e-7ae2-4fed-9ab6-38a95bf0504c","executionInfo":{"status":"ok","timestamp":1541818921800,"user_tz":-480,"elapsed":740,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# identify that that there are indeed much more non-fraudulent transactions in the dataset\n","df.Class.value_counts()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    284315\n","1       492\n","Name: Class, dtype: int64"]},"metadata":{"tags":[]},"execution_count":6}]},{"metadata":{"id":"ch0y5lVZR7DN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f32c7852-360a-4c52-cc07-ae5841655b10","executionInfo":{"status":"ok","timestamp":1541818923871,"user_tz":-480,"elapsed":754,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# ratio is approximately 0.0017 fraudulent : 1 non-fraudulent\n","df.Class.value_counts()[1] / (df.Class.value_counts()[0] + df.Class.value_counts()[1])"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.001727485630620034"]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"id":"pbbK_D_hQvo1","colab_type":"text"},"cell_type":"markdown","source":["### 2) Pre-process dataset"]},{"metadata":{"id":"BeCU0zNrQzB7","colab_type":"text"},"cell_type":"markdown","source":["#### 2a) Obtain validation and test data from temporal (time-series) dataset"]},{"metadata":{"id":"kWKh3O1m5GAl","colab_type":"code","colab":{}},"cell_type":"code","source":["# remove time column\n","df.drop('Time', inplace=True, axis=1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9a_B87oS1J5_","colab_type":"code","colab":{}},"cell_type":"code","source":["# split temporal data into training, validation and testing\n","# use 6:2:2 ratio\n","X = df.loc[:, df.columns!='Class']\n","y = df.loc[:, df.columns=='Class']\n","X_train = X.iloc[:170884, :] \n","y_train = y.iloc[:170884, :]\n","X_val = X.iloc[170884:227844, :] \n","y_val = y.iloc[170884:227844, :] \n","X_test = X.iloc[227844:, :] \n","y_test = y.iloc[227844:, :]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"63piDKReSyz7","colab_type":"text"},"cell_type":"markdown","source":["#### 2b) Data normalisation"]},{"metadata":{"id":"N7tO-RDr5Rfa","colab_type":"code","colab":{}},"cell_type":"code","source":["# scale all features such that they have mean 0 and unit variance\n","# to prevent data leakage, only use fit_transform for train data\n","std_scaler = StandardScaler()\n","X_train = std_scaler.fit_transform(X_train)\n","X_val = std_scaler.transform(X_val)\n","X_test = std_scaler.transform(X_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hNawBTuXSwgP","colab_type":"text"},"cell_type":"markdown","source":["### 3) Preliminary models on imbalanced data"]},{"metadata":{"id":"1m7rcSLcTHyK","colab_type":"text"},"cell_type":"markdown","source":["#### 3a) Logistic regression"]},{"metadata":{"id":"pktY257Z4L2l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"22312381-5aa8-4e3d-b4d2-fa0b6ff8a452","executionInfo":{"status":"ok","timestamp":1540039971252,"user_tz":-480,"elapsed":3791,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# logistic regression\n","from sklearn.linear_model import LogisticRegression\n","logreg = LogisticRegression()\n","logreg.fit(X_train, y_train)\n","y_pred = logreg.predict(X_test)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print('Results on test set...')\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[56874    14]\n"," [   35    40]]\n","Classification accuracy: 0.9991397924968839\n","Sensitivity: 0.5333333333333333\n","Precision: 0.7407407407407407\n","F2 score: 0.5649717514124293\n"],"name":"stdout"}]},{"metadata":{"id":"ZpjpF4HNTW_W","colab_type":"text"},"cell_type":"markdown","source":["#### 3b) Naive bayes"]},{"metadata":{"id":"Oqy-NwBw4L4Z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"d1baa50b-a6e1-41dd-e484-ff03550095b0","executionInfo":{"status":"ok","timestamp":1540039976031,"user_tz":-480,"elapsed":1761,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# naive bayes\n","from sklearn.naive_bayes import GaussianNB\n","nb = GaussianNB()\n","nb.fit(X_train, y_train)\n","y_pred = nb.predict(X_test)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print('Results on test set...')\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[55721  1167]\n"," [   15    60]]\n","Classification accuracy: 0.9792496883942208\n","Sensitivity: 0.8\n","Precision: 0.0488997555012225\n","F2 score: 0.19646365422396858\n"],"name":"stdout"}]},{"metadata":{"id":"Dfnh8OcwTZDt","colab_type":"text"},"cell_type":"markdown","source":["#### 3c) Decision tree"]},{"metadata":{"id":"FkThSZQ94L6R","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"2820b626-db3b-4216-d077-60f627c00c75","executionInfo":{"status":"ok","timestamp":1540039989796,"user_tz":-480,"elapsed":12148,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# decision tree\n","from sklearn.tree import DecisionTreeClassifier\n","dt = DecisionTreeClassifier(random_state=0)\n","dt.fit(X_train, y_train)\n","y_pred = dt.predict(X_test)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print('Results on test set...')\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[56837    51]\n"," [   23    52]]\n","Classification accuracy: 0.9987009111177431\n","Sensitivity: 0.6933333333333334\n","Precision: 0.5048543689320388\n","F2 score: 0.6451612903225806\n"],"name":"stdout"}]},{"metadata":{"id":"7OuQmAEMTb51","colab_type":"text"},"cell_type":"markdown","source":["#### 3d) Conclusion"]},{"metadata":{"id":"6pz_FiM4TfHk","colab_type":"text"},"cell_type":"markdown","source":["The trained models have arbitrarily high classification accuracy. This is a consequence of imbalanced class. "]},{"metadata":{"id":"NrOzaCqDTuZa","colab_type":"text"},"cell_type":"markdown","source":["### 4) Oversample minority class with SMOTE"]},{"metadata":{"id":"NlrBY5FpT1Hb","colab_type":"text"},"cell_type":"markdown","source":["#### 4a) SMOTE"]},{"metadata":{"id":"ddHchli8Ty-x","colab_type":"code","colab":{}},"cell_type":"code","source":["# apply SMOTE to train data\n","from imblearn.over_sampling import SMOTE\n","sm = SMOTE(ratio='minority')\n","X_smtrain, y_smtrain = sm.fit_sample(X_train, y_train.values.ravel())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VPvw3TDYT7Q4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"759956b1-0ae3-454b-b2e4-3a6d3b739119","executionInfo":{"status":"ok","timestamp":1541819772489,"user_tz":-480,"elapsed":945,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# verify that after sampling, the classes are equal\n","import collections\n","collections.Counter(y_smtrain)"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Counter({0: 170524, 1: 170524})"]},"metadata":{"tags":[]},"execution_count":13}]},{"metadata":{"id":"IwkZzFEdUBlB","colab_type":"text"},"cell_type":"markdown","source":["#### 4b) Logistic regression"]},{"metadata":{"id":"eYxs4ca-UEFl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"3ad9551d-5812-4163-a1ef-d6728dec974c","executionInfo":{"status":"ok","timestamp":1540040003833,"user_tz":-480,"elapsed":7155,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# logistic regression\n","from sklearn.linear_model import LogisticRegression\n","logreg = LogisticRegression()\n","logreg.fit(X_smtrain, y_smtrain)\n","y_pred = logreg.predict(X_test)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print('Results on test set...')\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[55542  1346]\n"," [    8    67]]\n","Classification accuracy: 0.9762301845057318\n","Sensitivity: 0.8933333333333333\n","Precision: 0.047416843595187545\n","F2 score: 0.19556333917104496\n"],"name":"stdout"}]},{"metadata":{"id":"vIhEONijUG8b","colab_type":"text"},"cell_type":"markdown","source":["#### 4c) K nearest neighbours"]},{"metadata":{"id":"JxCwrCIQULuL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"3790ced9-1435-42d4-9352-6cca3fd3bee4","executionInfo":{"status":"ok","timestamp":1540040290260,"user_tz":-480,"elapsed":261964,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# knn\n","from sklearn.neighbors import KNeighborsClassifier\n","knn = KNeighborsClassifier(n_neighbors=1)\n","knn.fit(X_smtrain, y_smtrain)\n","y_pred = knn.predict(X_test)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print('Results on test set...')\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[56855    33]\n"," [   15    60]]\n","Classification accuracy: 0.9991573477520496\n","Sensitivity: 0.8\n","Precision: 0.6451612903225806\n","F2 score: 0.7633587786259541\n"],"name":"stdout"}]},{"metadata":{"id":"F7sHOQO3UOUm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":952},"outputId":"025caaa6-21ce-4807-f87e-d217a0a8eaa1","executionInfo":{"status":"ok","timestamp":1540044618041,"user_tz":-480,"elapsed":4327697,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# finding the best value of k\n","print('Results on validation set...')\n","neighbours = range(1, 10, 1)\n","\n","for n_neighbour in neighbours:\n","  knn = KNeighborsClassifier(n_neighbors=n_neighbour)\n","  knn.fit(X_smtrain, y_smtrain)\n","  print('\\nValue of K is %d...' % n_neighbour)\n","  y_pred = knn.predict(X_val)\n","  confusion = confusion_matrix(y_val, y_pred)\n","  TP = confusion[1][1]\n","  TN = confusion[0][0]\n","  FP = confusion[0][1]\n","  FN = confusion[1][0]\n","  sensitivity = TP / float(TP + FN)\n","  precision = TP / float(TP + FP)\n","  print('Classification accuracy:', accuracy_score(y_val, y_pred))\n","  print('Sensitivity:', sensitivity)\n","  print('Precision:', precision)\n","  print('F2 score:', fbeta_score(y_val, y_pred, 2))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Results on validation set...\n","\n","Value of K is 1...\n","Classification accuracy: 0.9990695224719102\n","Sensitivity: 0.7543859649122807\n","Precision: 0.524390243902439\n","F2 score: 0.6935483870967742\n","\n","Value of K is 2...\n","Classification accuracy: 0.9990695224719102\n","Sensitivity: 0.7543859649122807\n","Precision: 0.524390243902439\n","F2 score: 0.6935483870967742\n","\n","Value of K is 3...\n","Classification accuracy: 0.9984901685393258\n","Sensitivity: 0.7543859649122807\n","Precision: 0.3739130434782609\n","F2 score: 0.6268221574344024\n","\n","Value of K is 4...\n","Classification accuracy: 0.9984901685393258\n","Sensitivity: 0.7543859649122807\n","Precision: 0.3739130434782609\n","F2 score: 0.6268221574344024\n","\n","Value of K is 5...\n","Classification accuracy: 0.9979108146067416\n","Sensitivity: 0.7894736842105263\n","Precision: 0.29605263157894735\n","F2 score: 0.5921052631578947\n","\n","Value of K is 6...\n","Classification accuracy: 0.9979459269662921\n","Sensitivity: 0.7894736842105263\n","Precision: 0.3\n","F2 score: 0.5952380952380952\n","\n","Value of K is 7...\n","Classification accuracy: 0.9974192415730337\n","Sensitivity: 0.7894736842105263\n","Precision: 0.25\n","F2 score: 0.5514705882352942\n","\n","Value of K is 8...\n","Classification accuracy: 0.997436797752809\n","Sensitivity: 0.7894736842105263\n","Precision: 0.25139664804469275\n","F2 score: 0.5528255528255528\n","\n","Value of K is 9...\n","Classification accuracy: 0.9965765449438202\n","Sensitivity: 0.7894736842105263\n","Precision: 0.19736842105263158\n","F2 score: 0.493421052631579\n"],"name":"stdout"}]},{"metadata":{"id":"7BrfetthUOaf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"01a95826-a209-4b0b-fe71-45ea6f8a0e03","executionInfo":{"status":"ok","timestamp":1540045031276,"user_tz":-480,"elapsed":390687,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# knn with k=2\n","knn = KNeighborsClassifier(n_neighbors=2)\n","knn.fit(X_smtrain, y_smtrain)\n","y_pred = knn.predict(X_test)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print('Results on test set...')\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[56856    32]\n"," [   15    60]]\n","Classification accuracy: 0.9991749030072152\n","Sensitivity: 0.8\n","Precision: 0.6521739130434783\n","F2 score: 0.7653061224489798\n"],"name":"stdout"}]},{"metadata":{"id":"wMTdv-i6UR0W","colab_type":"text"},"cell_type":"markdown","source":["#### 4d) Naive bayes"]},{"metadata":{"id":"PSweFdWJUUv8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"eb804573-1452-4e24-f5cd-89a74d134c19","executionInfo":{"status":"ok","timestamp":1540045032938,"user_tz":-480,"elapsed":1615,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# naive bayes\n","from sklearn.naive_bayes import GaussianNB\n","nb = GaussianNB()\n","nb.fit(X_smtrain, y_smtrain)\n","y_pred = nb.predict(X_test)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print('Results on test set...')\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[55625  1263]\n"," [   14    61]]\n","Classification accuracy: 0.9775819391534856\n","Sensitivity: 0.8133333333333334\n","Precision: 0.04607250755287009\n","F2 score: 0.187807881773399\n"],"name":"stdout"}]},{"metadata":{"id":"BfnLuoxEUVaF","colab_type":"text"},"cell_type":"markdown","source":["#### 4e) Decision tree"]},{"metadata":{"id":"m6SIGheZUoK5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"cedc8052-6888-478f-bad1-96d11f512c38","executionInfo":{"status":"ok","timestamp":1540045067413,"user_tz":-480,"elapsed":34387,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# decision tree\n","from sklearn.tree import DecisionTreeClassifier\n","dt = DecisionTreeClassifier(random_state=0)\n","dt.fit(X_smtrain, y_smtrain)\n","y_pred = dt.predict(X_test)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print('Results on test set...')\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[56736   152]\n"," [   23    52]]\n","Classification accuracy: 0.9969278303460141\n","Sensitivity: 0.6933333333333334\n","Precision: 0.2549019607843137\n","F2 score: 0.5158730158730158\n"],"name":"stdout"}]},{"metadata":{"id":"ZqjVkD3TUo_6","colab_type":"text"},"cell_type":"markdown","source":["#### 4f) Random forest"]},{"metadata":{"id":"tr1S2gOGUscb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"2f0ac6e0-a5ba-4c8a-a2bd-2c8a135813b7","executionInfo":{"status":"ok","timestamp":1540045090651,"user_tz":-480,"elapsed":23193,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# random forest\n","from sklearn.ensemble import RandomForestClassifier\n","rf = RandomForestClassifier(random_state=0)\n","rf.fit(X_smtrain, y_smtrain)\n","y_pred = rf.predict(X_test)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print('Results on test set...')\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[56884     4]\n"," [   22    53]]\n","Classification accuracy: 0.9995435633656935\n","Sensitivity: 0.7066666666666667\n","Precision: 0.9298245614035088\n","F2 score: 0.7422969187675069\n"],"name":"stdout"}]},{"metadata":{"id":"m8eSwA1rUt_U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":952},"outputId":"c36d0243-1e5b-455e-da84-f422a65dbe91","executionInfo":{"status":"ok","timestamp":1540045192848,"user_tz":-480,"elapsed":102182,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# finding the optimal number of trees in random forest\n","print('Results on validation set...')\n","num_estimators = range(1, 10, 1)\n","\n","for num_tree in num_estimators:\n","  rf = RandomForestClassifier(random_state=0, n_estimators=num_tree)\n","  rf.fit(X_smtrain, y_smtrain)\n","  print('\\nNumber of decision trees in random forest is %d...' % num_tree)\n","  y_pred = rf.predict(X_val)\n","  confusion = confusion_matrix(y_val, y_pred)\n","  TP = confusion[1][1]\n","  TN = confusion[0][0]\n","  FP = confusion[0][1]\n","  FN = confusion[1][0]\n","  sensitivity = TP / float(TP + FN)\n","  precision = TP / float(TP + FP)\n","  print('Classification accuracy:', accuracy_score(y_val, y_pred))\n","  print('Sensitivity:', sensitivity)\n","  print('Precision:', precision)\n","  print('F2 score:', fbeta_score(y_val, y_pred, 2))"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Results on validation set...\n","\n","Number of decision trees in random forest is 1...\n","Classification accuracy: 0.9937675561797753\n","Sensitivity: 0.631578947368421\n","Precision: 0.0972972972972973\n","F2 score: 0.3010033444816054\n","\n","Number of decision trees in random forest is 2...\n","Classification accuracy: 0.9994030898876405\n","Sensitivity: 0.5263157894736842\n","Precision: 0.8108108108108109\n","F2 score: 0.5660377358490567\n","\n","Number of decision trees in random forest is 3...\n","Classification accuracy: 0.999122191011236\n","Sensitivity: 0.6140350877192983\n","Precision: 0.5555555555555556\n","F2 score: 0.6013745704467354\n","\n","Number of decision trees in random forest is 4...\n","Classification accuracy: 0.9995259831460674\n","Sensitivity: 0.5964912280701754\n","Precision: 0.8947368421052632\n","F2 score: 0.6390977443609023\n","\n","Number of decision trees in random forest is 5...\n","Classification accuracy: 0.999561095505618\n","Sensitivity: 0.6491228070175439\n","Precision: 0.8809523809523809\n","F2 score: 0.6851851851851853\n","\n","Number of decision trees in random forest is 6...\n","Classification accuracy: 0.999561095505618\n","Sensitivity: 0.6140350877192983\n","Precision: 0.9210526315789473\n","F2 score: 0.6578947368421053\n","\n","Number of decision trees in random forest is 7...\n","Classification accuracy: 0.9995962078651686\n","Sensitivity: 0.6666666666666666\n","Precision: 0.9047619047619048\n","F2 score: 0.7037037037037037\n","\n","Number of decision trees in random forest is 8...\n","Classification accuracy: 0.9996137640449438\n","Sensitivity: 0.6666666666666666\n","Precision: 0.926829268292683\n","F2 score: 0.7063197026022305\n","\n","Number of decision trees in random forest is 9...\n","Classification accuracy: 0.9995962078651686\n","Sensitivity: 0.6666666666666666\n","Precision: 0.9047619047619048\n","F2 score: 0.7037037037037037\n"],"name":"stdout"}]},{"metadata":{"id":"ymtKVBPQUuCj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"5eec550f-9d21-454c-82d2-cd26ed90ec26","executionInfo":{"status":"ok","timestamp":1540045397491,"user_tz":-480,"elapsed":28087,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# random forest with number of trees set at 8\n","rf = RandomForestClassifier(random_state=0, n_estimators=8)\n","rf.fit(X_smtrain, y_smtrain)\n","y_pred = rf.predict(X_test)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print('Results on test set...')\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[56882     6]\n"," [   22    53]]\n","Classification accuracy: 0.9995084528553623\n","Sensitivity: 0.7066666666666667\n","Precision: 0.8983050847457628\n","F2 score: 0.7381615598885793\n"],"name":"stdout"}]},{"metadata":{"id":"4hJ3stYKUzXa","colab_type":"text"},"cell_type":"markdown","source":["#### 4g) Single layer perceptron"]},{"metadata":{"id":"YLT_At7VU2hN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"87904e4b-c50a-4029-bb79-0b3c21a574f9","executionInfo":{"status":"ok","timestamp":1540045502000,"user_tz":-480,"elapsed":104463,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# single layer perceptron\n","from sklearn.linear_model import Perceptron\n","pct = Perceptron(tol=None, max_iter=1000)\n","pct.fit(X_smtrain, y_smtrain)\n","y_pred = pct.predict(X_test)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print('Results on test set...')\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[51639  5249]\n"," [    6    69]]\n","Classification accuracy: 0.9077471341045942\n","Sensitivity: 0.92\n","Precision: 0.012974802557352389\n","F2 score: 0.06140975436098255\n"],"name":"stdout"}]},{"metadata":{"id":"XnDC0pWoU881","colab_type":"text"},"cell_type":"markdown","source":["#### 4h) Multi layer perceptron"]},{"metadata":{"id":"-g-aFy8TU6Ny","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1071},"outputId":"a4eef78d-5232-44fc-ed59-af62bfdff07a","executionInfo":{"status":"ok","timestamp":1540045568287,"user_tz":-480,"elapsed":66252,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# after some trials, mlp with 3 hidden layers appear to perform the best\n","from sklearn.neural_network import MLPClassifier\n","mlp = MLPClassifier(solver='sgd', # stochastic gradient descent\n","                    hidden_layer_sizes=(20,20,5), verbose=10, # state log\n","                    random_state=0, max_iter=1000, tol=1e-4) # set tolerance as 1e-4\n","mlp.fit(X_smtrain, y_smtrain)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Iteration 1, loss = 0.18973181\n","Iteration 2, loss = 0.10231158\n","Iteration 3, loss = 0.07750280\n","Iteration 4, loss = 0.06081228\n","Iteration 5, loss = 0.04771186\n","Iteration 6, loss = 0.03734229\n","Iteration 7, loss = 0.02990838\n","Iteration 8, loss = 0.02471018\n","Iteration 9, loss = 0.02118374\n","Iteration 10, loss = 0.01863738\n","Iteration 11, loss = 0.01664201\n","Iteration 12, loss = 0.01497504\n","Iteration 13, loss = 0.01357816\n","Iteration 14, loss = 0.01238342\n","Iteration 15, loss = 0.01139431\n","Iteration 16, loss = 0.01048671\n","Iteration 17, loss = 0.00979465\n","Iteration 18, loss = 0.00910163\n","Iteration 19, loss = 0.00852811\n","Iteration 20, loss = 0.00803835\n","Iteration 21, loss = 0.00763038\n","Iteration 22, loss = 0.00722934\n","Iteration 23, loss = 0.00688670\n","Iteration 24, loss = 0.00659646\n","Iteration 25, loss = 0.00635062\n","Iteration 26, loss = 0.00609455\n","Iteration 27, loss = 0.00589926\n","Iteration 28, loss = 0.00567958\n","Iteration 29, loss = 0.00545096\n","Iteration 30, loss = 0.00530347\n","Iteration 31, loss = 0.00513776\n","Iteration 32, loss = 0.00501486\n","Iteration 33, loss = 0.00486232\n","Iteration 34, loss = 0.00474354\n","Iteration 35, loss = 0.00461912\n","Iteration 36, loss = 0.00450092\n","Iteration 37, loss = 0.00441503\n","Iteration 38, loss = 0.00429762\n","Iteration 39, loss = 0.00419588\n","Iteration 40, loss = 0.00410303\n","Iteration 41, loss = 0.00401725\n","Iteration 42, loss = 0.00396837\n","Iteration 43, loss = 0.00384417\n","Iteration 44, loss = 0.00381017\n","Iteration 45, loss = 0.00371250\n","Iteration 46, loss = 0.00364930\n","Iteration 47, loss = 0.00359520\n","Iteration 48, loss = 0.00350357\n","Iteration 49, loss = 0.00352814\n","Iteration 50, loss = 0.00342447\n","Iteration 51, loss = 0.00337830\n","Iteration 52, loss = 0.00333352\n","Iteration 53, loss = 0.00325649\n","Iteration 54, loss = 0.00323068\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n","       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n","       hidden_layer_sizes=(20, 20, 5), learning_rate='constant',\n","       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n","       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n","       random_state=0, shuffle=True, solver='sgd', tol=0.0001,\n","       validation_fraction=0.1, verbose=10, warm_start=False)"]},"metadata":{"tags":[]},"execution_count":27}]},{"metadata":{"id":"-v56dCZVU6UI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"450eaa62-8f1b-4b07-80aa-773f49a3bb61","executionInfo":{"status":"ok","timestamp":1540045569877,"user_tz":-480,"elapsed":1548,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# performance on validation data\n","print('Results on validation set...')\n","y_pred = mlp.predict(X_val)\n","confusion = confusion_matrix(y_val, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_val, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_val, y_pred, 2))"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Results on validation set...\n","[[56832    71]\n"," [   17    40]]\n","Classification accuracy: 0.9984550561797753\n","Sensitivity: 0.7017543859649122\n","Precision: 0.36036036036036034\n","F2 score: 0.5899705014749261\n"],"name":"stdout"}]},{"metadata":{"id":"oVYefxjUVbLg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1071},"outputId":"fa767a2b-de0a-4280-8c98-310397ba47b7","executionInfo":{"status":"ok","timestamp":1540045801253,"user_tz":-480,"elapsed":77683,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# l2 regularisation\n","mlp = MLPClassifier(solver='sgd', # stochastic gradient descent\n","                    alpha=0.0003, # regularisation\n","                    hidden_layer_sizes=(20,20,5), verbose=10, # state log\n","                    random_state=0, max_iter=1000, tol=1e-4) # set tolerance as 1e-4\n","mlp.fit(X_smtrain, y_smtrain)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Iteration 1, loss = 0.18976231\n","Iteration 2, loss = 0.10234649\n","Iteration 3, loss = 0.07753975\n","Iteration 4, loss = 0.06084899\n","Iteration 5, loss = 0.04774942\n","Iteration 6, loss = 0.03738147\n","Iteration 7, loss = 0.02994713\n","Iteration 8, loss = 0.02475126\n","Iteration 9, loss = 0.02122413\n","Iteration 10, loss = 0.01867855\n","Iteration 11, loss = 0.01668408\n","Iteration 12, loss = 0.01501937\n","Iteration 13, loss = 0.01362037\n","Iteration 14, loss = 0.01242778\n","Iteration 15, loss = 0.01143727\n","Iteration 16, loss = 0.01052656\n","Iteration 17, loss = 0.00983384\n","Iteration 18, loss = 0.00913644\n","Iteration 19, loss = 0.00856593\n","Iteration 20, loss = 0.00807769\n","Iteration 21, loss = 0.00767520\n","Iteration 22, loss = 0.00727923\n","Iteration 23, loss = 0.00695165\n","Iteration 24, loss = 0.00664877\n","Iteration 25, loss = 0.00640224\n","Iteration 26, loss = 0.00616972\n","Iteration 27, loss = 0.00595950\n","Iteration 28, loss = 0.00575064\n","Iteration 29, loss = 0.00551585\n","Iteration 30, loss = 0.00536478\n","Iteration 31, loss = 0.00520049\n","Iteration 32, loss = 0.00508850\n","Iteration 33, loss = 0.00491591\n","Iteration 34, loss = 0.00479325\n","Iteration 35, loss = 0.00468181\n","Iteration 36, loss = 0.00457228\n","Iteration 37, loss = 0.00447797\n","Iteration 38, loss = 0.00435772\n","Iteration 39, loss = 0.00425019\n","Iteration 40, loss = 0.00416056\n","Iteration 41, loss = 0.00407585\n","Iteration 42, loss = 0.00403008\n","Iteration 43, loss = 0.00390273\n","Iteration 44, loss = 0.00386933\n","Iteration 45, loss = 0.00377200\n","Iteration 46, loss = 0.00370945\n","Iteration 47, loss = 0.00365545\n","Iteration 48, loss = 0.00356428\n","Iteration 49, loss = 0.00360449\n","Iteration 50, loss = 0.00349725\n","Iteration 51, loss = 0.00344323\n","Iteration 52, loss = 0.00339361\n","Iteration 53, loss = 0.00331442\n","Iteration 54, loss = 0.00327457\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["MLPClassifier(activation='relu', alpha=0.0003, batch_size='auto', beta_1=0.9,\n","       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n","       hidden_layer_sizes=(20, 20, 5), learning_rate='constant',\n","       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n","       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n","       random_state=0, shuffle=True, solver='sgd', tol=0.0001,\n","       validation_fraction=0.1, verbose=10, warm_start=False)"]},"metadata":{"tags":[]},"execution_count":31}]},{"metadata":{"id":"XARUfmhIVisq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"bfd53d75-4566-41b4-d5ed-a3896eb07b22","executionInfo":{"status":"ok","timestamp":1540045896034,"user_tz":-480,"elapsed":997,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# performance on validation data\n","print('Results on validation set...')\n","y_pred = mlp.predict(X_val)\n","confusion = confusion_matrix(y_val, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_val, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_val, y_pred, 2))"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Results on validation set...\n","[[56832    71]\n"," [   17    40]]\n","Classification accuracy: 0.9984550561797753\n","Sensitivity: 0.7017543859649122\n","Precision: 0.36036036036036034\n","F2 score: 0.5899705014749261\n"],"name":"stdout"}]},{"metadata":{"id":"PgB_rWR6VLbv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"94c42e60-f38b-4d79-9d60-48dd70c52991","executionInfo":{"status":"ok","timestamp":1540045911605,"user_tz":-480,"elapsed":1022,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# performance on test data\n","print('Results on test set...')\n","y_pred = mlp.predict(X_test)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[56825    63]\n"," [   16    59]]\n","Classification accuracy: 0.9986131348419149\n","Sensitivity: 0.7866666666666666\n","Precision: 0.48360655737704916\n","F2 score: 0.6990521327014219\n"],"name":"stdout"}]},{"metadata":{"id":"DMDsBWfKeEMb","colab_type":"text"},"cell_type":"markdown","source":["### 5) Undersample majority class with Near Miss"]},{"metadata":{"id":"5k67G_3xeSnv","colab_type":"text"},"cell_type":"markdown","source":["#### 5a) Near Miss"]},{"metadata":{"id":"Fk4zc6aZVY4y","colab_type":"code","colab":{}},"cell_type":"code","source":["# apply near miss to train data\n","from imblearn.under_sampling import NearMiss\n","nm = NearMiss()\n","X_nmtrain, y_nmtrain = nm.fit_sample(X_train, y_train.values.ravel())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_lkB67IyVY7p","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"42bf5cf6-e818-446f-ec3c-c313ac482e9a","executionInfo":{"status":"ok","timestamp":1540045927373,"user_tz":-480,"elapsed":2562,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# verify that after sampling, the classes are equal\n","import collections\n","collections.Counter(y_nmtrain)"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Counter({0: 360, 1: 360})"]},"metadata":{"tags":[]},"execution_count":35}]},{"metadata":{"id":"beiyDdIWeiH8","colab_type":"text"},"cell_type":"markdown","source":["#### 5b) Logistic regression"]},{"metadata":{"id":"KDZR8I_ZerHW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"e775c983-7591-4a1b-82ea-3e4579ea190f","executionInfo":{"status":"ok","timestamp":1540045932090,"user_tz":-480,"elapsed":1577,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# logistic regression\n","from sklearn.linear_model import LogisticRegression\n","logreg = LogisticRegression()\n","logreg.fit(X_nmtrain, y_nmtrain)\n","y_pred = logreg.predict(X_test)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print('Results on test set...')\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":36,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[36549 20339]\n"," [    2    73]]\n","Classification accuracy: 0.6429085546758422\n","Sensitivity: 0.9733333333333334\n","Precision: 0.0035763276504017245\n","F2 score: 0.01762263422170722\n"],"name":"stdout"}]},{"metadata":{"id":"WR97sWxpgawk","colab_type":"text"},"cell_type":"markdown","source":["#### 5c) K nearest neighbours"]},{"metadata":{"id":"-FYEa5V0erKc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"626fff12-3542-4096-f968-318fce208f68","executionInfo":{"status":"ok","timestamp":1540045937090,"user_tz":-480,"elapsed":2922,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# knn\n","from sklearn.neighbors import KNeighborsClassifier\n","knn = KNeighborsClassifier(n_neighbors=1)\n","knn.fit(X_nmtrain, y_nmtrain)\n","y_pred = knn.predict(X_test)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print('Results on test set...')\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[29798 27090]\n"," [    4    71]]\n","Classification accuracy: 0.5243579165423169\n","Sensitivity: 0.9466666666666667\n","Precision: 0.0026140421928500424\n","F2 score: 0.012927424347256107\n"],"name":"stdout"}]},{"metadata":{"id":"1Zae93zuerNq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":952},"outputId":"38661b39-bb13-49fd-97b6-1ab3f586886f","executionInfo":{"status":"ok","timestamp":1540045962094,"user_tz":-480,"elapsed":18769,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# finding the best value of k\n","print('Results on validation set...')\n","neighbours = range(1, 10, 1)\n","\n","for n_neighbour in neighbours:\n","  knn = KNeighborsClassifier(n_neighbors=n_neighbour)\n","  knn.fit(X_nmtrain, y_nmtrain)\n","  print('\\nValue of K is %d...' % n_neighbour)\n","  y_pred = knn.predict(X_val)\n","  confusion = confusion_matrix(y_val, y_pred)\n","  TP = confusion[1][1]\n","  TN = confusion[0][0]\n","  FP = confusion[0][1]\n","  FN = confusion[1][0]\n","  sensitivity = TP / float(TP + FN)\n","  precision = TP / float(TP + FP)\n","  print('Classification accuracy:', accuracy_score(y_val, y_pred))\n","  print('Sensitivity:', sensitivity)\n","  print('Precision:', precision)\n","  print('F2 score:', fbeta_score(y_val, y_pred, 2))"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Results on validation set...\n","\n","Value of K is 1...\n","Classification accuracy: 0.48491924157303373\n","Sensitivity: 0.9298245614035088\n","Precision: 0.0018034571934122771\n","F2 score: 0.008947866018368451\n","\n","Value of K is 2...\n","Classification accuracy: 0.7567591292134831\n","Sensitivity: 0.9122807017543859\n","Precision: 0.0037404689972665803\n","F2 score: 0.01840056617126681\n","\n","Value of K is 3...\n","Classification accuracy: 0.6857619382022472\n","Sensitivity: 0.9298245614035088\n","Precision: 0.0029529752618676175\n","F2 score: 0.014579665492957748\n","\n","Value of K is 4...\n","Classification accuracy: 0.8262816011235955\n","Sensitivity: 0.9122807017543859\n","Precision: 0.005230335948501308\n","F2 score: 0.025565388397246806\n","\n","Value of K is 5...\n","Classification accuracy: 0.7912219101123595\n","Sensitivity: 0.9122807017543859\n","Precision: 0.0043554736577602815\n","F2 score: 0.021369277554039615\n","\n","Value of K is 6...\n","Classification accuracy: 0.8738939606741573\n","Sensitivity: 0.8947368421052632\n","Precision: 0.007055893746541229\n","F2 score: 0.03420064377682403\n","\n","Value of K is 7...\n","Classification accuracy: 0.8573033707865169\n","Sensitivity: 0.9122807017543859\n","Precision: 0.00636085626911315\n","F2 score: 0.030941330477210517\n","\n","Value of K is 8...\n","Classification accuracy: 0.9117626404494382\n","Sensitivity: 0.8947368421052632\n","Precision: 0.010057187931374483\n","F2 score: 0.04812228722400454\n","\n","Value of K is 9...\n","Classification accuracy: 0.9010884831460674\n","Sensitivity: 0.9122807017543859\n","Precision: 0.009153318077803204\n","F2 score: 0.04400067693349129\n"],"name":"stdout"}]},{"metadata":{"id":"16YmUoOWerUh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"86eee5ef-db81-4d42-d988-d1be7a850318","executionInfo":{"status":"ok","timestamp":1540045985400,"user_tz":-480,"elapsed":3429,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# knn with k=8\n","knn = KNeighborsClassifier(n_neighbors=8)\n","knn.fit(X_nmtrain, y_nmtrain)\n","y_pred = knn.predict(X_test)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print('Results on test set...')\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[52529  4359]\n"," [    8    67]]\n","Classification accuracy: 0.9233362006916771\n","Sensitivity: 0.8933333333333333\n","Precision: 0.015137821961138725\n","F2 score: 0.07088446889547187\n"],"name":"stdout"}]},{"metadata":{"id":"8DxGjRS-grKm","colab_type":"text"},"cell_type":"markdown","source":["#### 5d) Naive bayes"]},{"metadata":{"id":"PRFGyLZnernn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"15e21918-84a0-42d5-f9fd-0397e373ca0a","executionInfo":{"status":"ok","timestamp":1540045989324,"user_tz":-480,"elapsed":1547,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# naive bayes\n","from sklearn.naive_bayes import GaussianNB\n","nb = GaussianNB()\n","nb.fit(X_nmtrain, y_nmtrain)\n","y_pred = nb.predict(X_test)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print('Results on test set...')\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":40,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[25897 30991]\n"," [    5    70]]\n","Classification accuracy: 0.4558573108860137\n","Sensitivity: 0.9333333333333333\n","Precision: 0.0022536299539615595\n","F2 score: 0.011160358406938554\n"],"name":"stdout"}]},{"metadata":{"id":"8Rzk06xWgtQ4","colab_type":"text"},"cell_type":"markdown","source":["#### 5e) Decision tree"]},{"metadata":{"id":"eOFNDwv8er-_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"2947c541-c45e-492f-c9ab-431b5a9107f0","executionInfo":{"status":"ok","timestamp":1540045992499,"user_tz":-480,"elapsed":1635,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# decision tree\n","from sklearn.tree import DecisionTreeClassifier\n","dt = DecisionTreeClassifier(random_state=0)\n","dt.fit(X_nmtrain, y_nmtrain)\n","y_pred = dt.predict(X_test)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print('Results on test set...')\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":41,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[12650 44238]\n"," [    3    72]]\n","Classification accuracy: 0.22333795621719363\n","Sensitivity: 0.96\n","Precision: 0.0016249153689911983\n","F2 score: 0.008069939475453933\n"],"name":"stdout"}]},{"metadata":{"id":"APQojBRDg0Vy","colab_type":"text"},"cell_type":"markdown","source":["#### 5f) Random forest"]},{"metadata":{"id":"YxTw1Hy5esEB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"ae0b8747-747a-497c-9070-6d7550a5d046","executionInfo":{"status":"ok","timestamp":1540045996074,"user_tz":-480,"elapsed":1003,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# random forest\n","from sklearn.ensemble import RandomForestClassifier\n","rf = RandomForestClassifier(random_state=0)\n","rf.fit(X_nmtrain, y_nmtrain)\n","y_pred = rf.predict(X_test)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print('Results on test set...')\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[ 2550 54338]\n"," [    1    74]]\n","Classification accuracy: 0.046064989554623174\n","Sensitivity: 0.9866666666666667\n","Precision: 0.0013599941189443505\n","F2 score: 0.0067626846030121365\n"],"name":"stdout"}]},{"metadata":{"id":"Tlo70y-NesL1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":952},"outputId":"32b576a4-a0c2-461d-ab24-a0adeeb81d39","executionInfo":{"status":"ok","timestamp":1540045998763,"user_tz":-480,"elapsed":1603,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# finding the optimal number of trees in random forest\n","print('Results on validation set...')\n","num_estimators = range(1, 10, 1)\n","\n","for num_tree in num_estimators:\n","  rf = RandomForestClassifier(random_state=0, n_estimators=num_tree)\n","  rf.fit(X_nmtrain, y_nmtrain)\n","  print('\\nNumber of decision trees in random forest is %d...' % num_tree)\n","  y_pred = rf.predict(X_val)\n","  confusion = confusion_matrix(y_val, y_pred)\n","  TP = confusion[1][1]\n","  TN = confusion[0][0]\n","  FP = confusion[0][1]\n","  FN = confusion[1][0]\n","  sensitivity = TP / float(TP + FN)\n","  precision = TP / float(TP + FP)\n","  print('Classification accuracy:', accuracy_score(y_val, y_pred))\n","  print('Sensitivity:', sensitivity)\n","  print('Precision:', precision)\n","  print('F2 score:', fbeta_score(y_val, y_pred, 2))"],"execution_count":43,"outputs":[{"output_type":"stream","text":["Results on validation set...\n","\n","Number of decision trees in random forest is 1...\n","Classification accuracy: 0.20180828651685392\n","Sensitivity: 0.9473684210526315\n","Precision: 0.0011863959926179805\n","F2 score: 0.005902413431269675\n","\n","Number of decision trees in random forest is 2...\n","Classification accuracy: 0.24074789325842696\n","Sensitivity: 0.9298245614035088\n","Precision: 0.001224131559497413\n","F2 score: 0.0060885947982722175\n","\n","Number of decision trees in random forest is 3...\n","Classification accuracy: 0.07550912921348314\n","Sensitivity: 0.9824561403508771\n","Precision: 0.0010623363812269984\n","F2 score: 0.005288806618563711\n","\n","Number of decision trees in random forest is 4...\n","Classification accuracy: 0.1180126404494382\n","Sensitivity: 0.9649122807017544\n","Precision: 0.0010936350440436658\n","F2 score: 0.005443496506264968\n","\n","Number of decision trees in random forest is 5...\n","Classification accuracy: 0.03297050561797753\n","Sensitivity: 1.0\n","Precision: 0.0010337510654890368\n","F2 score: 0.005147470514927665\n","\n","Number of decision trees in random forest is 6...\n","Classification accuracy: 0.06337780898876405\n","Sensitivity: 0.9649122807017544\n","Precision: 0.001029904687002603\n","F2 score: 0.005127631407208518\n","\n","Number of decision trees in random forest is 7...\n","Classification accuracy: 0.020400280898876404\n","Sensitivity: 0.9824561403508771\n","Precision: 0.0010026319087604962\n","F2 score: 0.004992778302812004\n","\n","Number of decision trees in random forest is 8...\n","Classification accuracy: 0.030003511235955058\n","Sensitivity: 0.9824561403508771\n","Precision: 0.0010125483672657578\n","F2 score: 0.0050419562790362665\n","\n","Number of decision trees in random forest is 9...\n","Classification accuracy: 0.021558988764044943\n","Sensitivity: 1.0\n","Precision: 0.0010217067880765025\n","F2 score: 0.005087741221414927\n"],"name":"stdout"}]},{"metadata":{"id":"2cUEBjy9esRQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"6568e058-9cfa-4ea1-c5cf-8500291f1b95","executionInfo":{"status":"ok","timestamp":1540046006148,"user_tz":-480,"elapsed":1639,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# random forest with number of trees set at 7\n","rf = RandomForestClassifier(random_state=0, n_estimators=7)\n","rf.fit(X_nmtrain, y_nmtrain)\n","y_pred = rf.predict(X_test)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print('Results on test set...')\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":44,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[ 1116 55772]\n"," [    1    74]]\n","Classification accuracy: 0.02089075364710426\n","Sensitivity: 0.9866666666666667\n","Precision: 0.001325072520860939\n","F2 score: 0.006589961885085314\n"],"name":"stdout"}]},{"metadata":{"id":"yuSHEkJpg8MB","colab_type":"text"},"cell_type":"markdown","source":["#### 5g) Single layer perceptron"]},{"metadata":{"id":"G0y-ZICXesW9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"b134c1fe-53d8-4b25-8811-ed09517406ff","executionInfo":{"status":"ok","timestamp":1540046009455,"user_tz":-480,"elapsed":1557,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# single layer perceptron\n","from sklearn.linear_model import Perceptron\n","pct = Perceptron(tol=None, max_iter=1000)\n","pct.fit(X_nmtrain, y_nmtrain)\n","y_pred = pct.predict(X_test)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print('Results on test set...')\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":45,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[25824 31064]\n"," [    4    71]]\n","Classification accuracy: 0.45459333251408807\n","Sensitivity: 0.9466666666666667\n","Precision: 0.002280391841978481\n","F2 score: 0.011293144584062352\n"],"name":"stdout"}]},{"metadata":{"id":"oFgxn2QrhEH3","colab_type":"text"},"cell_type":"markdown","source":["#### 5h) Multi layer perceptron"]},{"metadata":{"id":"7V_vBj6Iesap","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":9010},"outputId":"d84e77da-5f5b-461a-caea-9ac1fca352ec","executionInfo":{"status":"ok","timestamp":1540046013705,"user_tz":-480,"elapsed":2711,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# after some trials, mlp with 3 hidden layers appear to perform the best\n","from sklearn.neural_network import MLPClassifier\n","mlp = MLPClassifier(solver='sgd', # stochastic gradient descent\n","                    hidden_layer_sizes=(20,20,5), verbose=10, # state log\n","                    random_state=0, max_iter=1000, tol=1e-4) # set tolerance as 1e-4\n","mlp.fit(X_nmtrain, y_nmtrain)"],"execution_count":46,"outputs":[{"output_type":"stream","text":["Iteration 1, loss = 0.77466037\n","Iteration 2, loss = 0.70151136\n","Iteration 3, loss = 0.63282393\n","Iteration 4, loss = 0.57582175\n","Iteration 5, loss = 0.52598895\n","Iteration 6, loss = 0.48641368\n","Iteration 7, loss = 0.45434683\n","Iteration 8, loss = 0.43206920\n","Iteration 9, loss = 0.41715298\n","Iteration 10, loss = 0.40627984\n","Iteration 11, loss = 0.39800800\n","Iteration 12, loss = 0.39225328\n","Iteration 13, loss = 0.38721154\n","Iteration 14, loss = 0.38317877\n","Iteration 15, loss = 0.37976953\n","Iteration 16, loss = 0.37664810\n","Iteration 17, loss = 0.37383303\n","Iteration 18, loss = 0.37129605\n","Iteration 19, loss = 0.36872169\n","Iteration 20, loss = 0.36639727\n","Iteration 21, loss = 0.36402682\n","Iteration 22, loss = 0.36204242\n","Iteration 23, loss = 0.35994929\n","Iteration 24, loss = 0.35791559\n","Iteration 25, loss = 0.35592690\n","Iteration 26, loss = 0.35402229\n","Iteration 27, loss = 0.35225155\n","Iteration 28, loss = 0.35030475\n","Iteration 29, loss = 0.34846671\n","Iteration 30, loss = 0.34670838\n","Iteration 31, loss = 0.34488775\n","Iteration 32, loss = 0.34309302\n","Iteration 33, loss = 0.34139918\n","Iteration 34, loss = 0.33961752\n","Iteration 35, loss = 0.33789926\n","Iteration 36, loss = 0.33615657\n","Iteration 37, loss = 0.33439305\n","Iteration 38, loss = 0.33263707\n","Iteration 39, loss = 0.33082903\n","Iteration 40, loss = 0.32895770\n","Iteration 41, loss = 0.32717472\n","Iteration 42, loss = 0.32514736\n","Iteration 43, loss = 0.32323861\n","Iteration 44, loss = 0.32128455\n","Iteration 45, loss = 0.31924794\n","Iteration 46, loss = 0.31725216\n","Iteration 47, loss = 0.31512416\n","Iteration 48, loss = 0.31296619\n","Iteration 49, loss = 0.31081299\n","Iteration 50, loss = 0.30855644\n","Iteration 51, loss = 0.30636681\n","Iteration 52, loss = 0.30428454\n","Iteration 53, loss = 0.30201740\n","Iteration 54, loss = 0.29977538\n","Iteration 55, loss = 0.29756682\n","Iteration 56, loss = 0.29532920\n","Iteration 57, loss = 0.29304988\n","Iteration 58, loss = 0.29081439\n","Iteration 59, loss = 0.28854016\n","Iteration 60, loss = 0.28628027\n","Iteration 61, loss = 0.28415939\n","Iteration 62, loss = 0.28195303\n","Iteration 63, loss = 0.27987826\n","Iteration 64, loss = 0.27781889\n","Iteration 65, loss = 0.27581083\n","Iteration 66, loss = 0.27388566\n","Iteration 67, loss = 0.27188785\n","Iteration 68, loss = 0.27003553\n","Iteration 69, loss = 0.26826630\n","Iteration 70, loss = 0.26642048\n","Iteration 71, loss = 0.26458340\n","Iteration 72, loss = 0.26284983\n","Iteration 73, loss = 0.26113236\n","Iteration 74, loss = 0.25938778\n","Iteration 75, loss = 0.25771626\n","Iteration 76, loss = 0.25608651\n","Iteration 77, loss = 0.25446558\n","Iteration 78, loss = 0.25285399\n","Iteration 79, loss = 0.25126436\n","Iteration 80, loss = 0.24973757\n","Iteration 81, loss = 0.24822146\n","Iteration 82, loss = 0.24669663\n","Iteration 83, loss = 0.24520080\n","Iteration 84, loss = 0.24378072\n","Iteration 85, loss = 0.24233229\n","Iteration 86, loss = 0.24091769\n","Iteration 87, loss = 0.23955695\n","Iteration 88, loss = 0.23831110\n","Iteration 89, loss = 0.23688028\n","Iteration 90, loss = 0.23553569\n","Iteration 91, loss = 0.23423642\n","Iteration 92, loss = 0.23297667\n","Iteration 93, loss = 0.23172423\n","Iteration 94, loss = 0.23049526\n","Iteration 95, loss = 0.22923752\n","Iteration 96, loss = 0.22803393\n","Iteration 97, loss = 0.22684368\n","Iteration 98, loss = 0.22565629\n","Iteration 99, loss = 0.22447824\n","Iteration 100, loss = 0.22328530\n","Iteration 101, loss = 0.22219237\n","Iteration 102, loss = 0.22105972\n","Iteration 103, loss = 0.21992380\n","Iteration 104, loss = 0.21880980\n","Iteration 105, loss = 0.21769090\n","Iteration 106, loss = 0.21655448\n","Iteration 107, loss = 0.21546313\n","Iteration 108, loss = 0.21437925\n","Iteration 109, loss = 0.21324379\n","Iteration 110, loss = 0.21220365\n","Iteration 111, loss = 0.21108588\n","Iteration 112, loss = 0.21001255\n","Iteration 113, loss = 0.20891979\n","Iteration 114, loss = 0.20784234\n","Iteration 115, loss = 0.20677203\n","Iteration 116, loss = 0.20568760\n","Iteration 117, loss = 0.20458948\n","Iteration 118, loss = 0.20352591\n","Iteration 119, loss = 0.20244914\n","Iteration 120, loss = 0.20139793\n","Iteration 121, loss = 0.20030106\n","Iteration 122, loss = 0.19914940\n","Iteration 123, loss = 0.19813074\n","Iteration 124, loss = 0.19706487\n","Iteration 125, loss = 0.19607350\n","Iteration 126, loss = 0.19495816\n","Iteration 127, loss = 0.19393117\n","Iteration 128, loss = 0.19283163\n","Iteration 129, loss = 0.19174085\n","Iteration 130, loss = 0.19074097\n","Iteration 131, loss = 0.18971063\n","Iteration 132, loss = 0.18865632\n","Iteration 133, loss = 0.18766752\n","Iteration 134, loss = 0.18661349\n","Iteration 135, loss = 0.18563983\n","Iteration 136, loss = 0.18465484\n","Iteration 137, loss = 0.18362664\n","Iteration 138, loss = 0.18264824\n","Iteration 139, loss = 0.18167999\n","Iteration 140, loss = 0.18069700\n","Iteration 141, loss = 0.17973753\n","Iteration 142, loss = 0.17879158\n","Iteration 143, loss = 0.17783208\n","Iteration 144, loss = 0.17685153\n","Iteration 145, loss = 0.17595620\n","Iteration 146, loss = 0.17499722\n","Iteration 147, loss = 0.17414545\n","Iteration 148, loss = 0.17314521\n","Iteration 149, loss = 0.17223112\n","Iteration 150, loss = 0.17134397\n","Iteration 151, loss = 0.17041044\n","Iteration 152, loss = 0.16951133\n","Iteration 153, loss = 0.16863204\n","Iteration 154, loss = 0.16775083\n","Iteration 155, loss = 0.16688090\n","Iteration 156, loss = 0.16606950\n","Iteration 157, loss = 0.16518070\n","Iteration 158, loss = 0.16430901\n","Iteration 159, loss = 0.16353579\n","Iteration 160, loss = 0.16260956\n","Iteration 161, loss = 0.16180987\n","Iteration 162, loss = 0.16100468\n","Iteration 163, loss = 0.16021780\n","Iteration 164, loss = 0.15936171\n","Iteration 165, loss = 0.15860152\n","Iteration 166, loss = 0.15775355\n","Iteration 167, loss = 0.15701735\n","Iteration 168, loss = 0.15625480\n","Iteration 169, loss = 0.15548792\n","Iteration 170, loss = 0.15469497\n","Iteration 171, loss = 0.15392533\n","Iteration 172, loss = 0.15324208\n","Iteration 173, loss = 0.15244091\n","Iteration 174, loss = 0.15165661\n","Iteration 175, loss = 0.15092701\n","Iteration 176, loss = 0.15021510\n","Iteration 177, loss = 0.14949573\n","Iteration 178, loss = 0.14884224\n","Iteration 179, loss = 0.14806119\n","Iteration 180, loss = 0.14738094\n","Iteration 181, loss = 0.14672818\n","Iteration 182, loss = 0.14599247\n","Iteration 183, loss = 0.14537462\n","Iteration 184, loss = 0.14467885\n","Iteration 185, loss = 0.14403330\n","Iteration 186, loss = 0.14339442\n","Iteration 187, loss = 0.14270678\n","Iteration 188, loss = 0.14207794\n","Iteration 189, loss = 0.14147487\n","Iteration 190, loss = 0.14085391\n","Iteration 191, loss = 0.14023669\n","Iteration 192, loss = 0.13963217\n","Iteration 193, loss = 0.13904099\n","Iteration 194, loss = 0.13843439\n","Iteration 195, loss = 0.13788089\n","Iteration 196, loss = 0.13729057\n","Iteration 197, loss = 0.13671992\n","Iteration 198, loss = 0.13617092\n","Iteration 199, loss = 0.13558983\n","Iteration 200, loss = 0.13503465\n","Iteration 201, loss = 0.13452150\n","Iteration 202, loss = 0.13397315\n","Iteration 203, loss = 0.13343988\n","Iteration 204, loss = 0.13292440\n","Iteration 205, loss = 0.13240754\n","Iteration 206, loss = 0.13185179\n","Iteration 207, loss = 0.13136983\n","Iteration 208, loss = 0.13083966\n","Iteration 209, loss = 0.13037495\n","Iteration 210, loss = 0.12987312\n","Iteration 211, loss = 0.12933855\n","Iteration 212, loss = 0.12888543\n","Iteration 213, loss = 0.12840406\n","Iteration 214, loss = 0.12788463\n","Iteration 215, loss = 0.12742038\n","Iteration 216, loss = 0.12695438\n","Iteration 217, loss = 0.12644380\n","Iteration 218, loss = 0.12600247\n","Iteration 219, loss = 0.12551358\n","Iteration 220, loss = 0.12500540\n","Iteration 221, loss = 0.12455982\n","Iteration 222, loss = 0.12409462\n","Iteration 223, loss = 0.12357389\n","Iteration 224, loss = 0.12314711\n","Iteration 225, loss = 0.12268579\n","Iteration 226, loss = 0.12220315\n","Iteration 227, loss = 0.12174943\n","Iteration 228, loss = 0.12126831\n","Iteration 229, loss = 0.12085464\n","Iteration 230, loss = 0.12039680\n","Iteration 231, loss = 0.12001131\n","Iteration 232, loss = 0.11960923\n","Iteration 233, loss = 0.11923013\n","Iteration 234, loss = 0.11885637\n","Iteration 235, loss = 0.11846599\n","Iteration 236, loss = 0.11812710\n","Iteration 237, loss = 0.11774180\n","Iteration 238, loss = 0.11740408\n","Iteration 239, loss = 0.11704606\n","Iteration 240, loss = 0.11666860\n","Iteration 241, loss = 0.11634162\n","Iteration 242, loss = 0.11597789\n","Iteration 243, loss = 0.11563539\n","Iteration 244, loss = 0.11529693\n","Iteration 245, loss = 0.11498856\n","Iteration 246, loss = 0.11467273\n","Iteration 247, loss = 0.11434101\n","Iteration 248, loss = 0.11402069\n","Iteration 249, loss = 0.11370056\n","Iteration 250, loss = 0.11339529\n","Iteration 251, loss = 0.11309853\n","Iteration 252, loss = 0.11278920\n","Iteration 253, loss = 0.11250869\n","Iteration 254, loss = 0.11214554\n","Iteration 255, loss = 0.11187165\n","Iteration 256, loss = 0.11156700\n","Iteration 257, loss = 0.11125515\n","Iteration 258, loss = 0.11097473\n","Iteration 259, loss = 0.11067789\n","Iteration 260, loss = 0.11034919\n","Iteration 261, loss = 0.11007373\n","Iteration 262, loss = 0.10980130\n","Iteration 263, loss = 0.10949472\n","Iteration 264, loss = 0.10922630\n","Iteration 265, loss = 0.10892413\n","Iteration 266, loss = 0.10866686\n","Iteration 267, loss = 0.10839466\n","Iteration 268, loss = 0.10812558\n","Iteration 269, loss = 0.10783369\n","Iteration 270, loss = 0.10758083\n","Iteration 271, loss = 0.10731176\n","Iteration 272, loss = 0.10705287\n","Iteration 273, loss = 0.10678555\n","Iteration 274, loss = 0.10652849\n","Iteration 275, loss = 0.10627691\n","Iteration 276, loss = 0.10602256\n","Iteration 277, loss = 0.10574136\n","Iteration 278, loss = 0.10551799\n","Iteration 279, loss = 0.10525865\n","Iteration 280, loss = 0.10501535\n","Iteration 281, loss = 0.10476707\n","Iteration 282, loss = 0.10454594\n","Iteration 283, loss = 0.10428252\n","Iteration 284, loss = 0.10398175\n","Iteration 285, loss = 0.10372783\n","Iteration 286, loss = 0.10349223\n","Iteration 287, loss = 0.10323011\n","Iteration 288, loss = 0.10297440\n","Iteration 289, loss = 0.10273511\n","Iteration 290, loss = 0.10250616\n","Iteration 291, loss = 0.10225359\n","Iteration 292, loss = 0.10200873\n","Iteration 293, loss = 0.10178500\n","Iteration 294, loss = 0.10154356\n","Iteration 295, loss = 0.10130331\n","Iteration 296, loss = 0.10108840\n","Iteration 297, loss = 0.10089320\n","Iteration 298, loss = 0.10061212\n","Iteration 299, loss = 0.10039924\n","Iteration 300, loss = 0.10016289\n","Iteration 301, loss = 0.09997092\n","Iteration 302, loss = 0.09974412\n","Iteration 303, loss = 0.09955601\n","Iteration 304, loss = 0.09932038\n","Iteration 305, loss = 0.09909126\n","Iteration 306, loss = 0.09887059\n","Iteration 307, loss = 0.09866655\n","Iteration 308, loss = 0.09841235\n","Iteration 309, loss = 0.09820968\n","Iteration 310, loss = 0.09802103\n","Iteration 311, loss = 0.09776899\n","Iteration 312, loss = 0.09757977\n","Iteration 313, loss = 0.09736024\n","Iteration 314, loss = 0.09713845\n","Iteration 315, loss = 0.09695036\n","Iteration 316, loss = 0.09674715\n","Iteration 317, loss = 0.09652857\n","Iteration 318, loss = 0.09631803\n","Iteration 319, loss = 0.09612551\n","Iteration 320, loss = 0.09590779\n","Iteration 321, loss = 0.09570760\n","Iteration 322, loss = 0.09550641\n","Iteration 323, loss = 0.09531467\n","Iteration 324, loss = 0.09513229\n","Iteration 325, loss = 0.09492634\n","Iteration 326, loss = 0.09472313\n","Iteration 327, loss = 0.09451967\n","Iteration 328, loss = 0.09434145\n","Iteration 329, loss = 0.09410848\n","Iteration 330, loss = 0.09392236\n","Iteration 331, loss = 0.09372775\n","Iteration 332, loss = 0.09349939\n","Iteration 333, loss = 0.09332148\n","Iteration 334, loss = 0.09311965\n","Iteration 335, loss = 0.09292668\n","Iteration 336, loss = 0.09268653\n","Iteration 337, loss = 0.09250777\n","Iteration 338, loss = 0.09232068\n","Iteration 339, loss = 0.09211551\n","Iteration 340, loss = 0.09193547\n","Iteration 341, loss = 0.09177412\n","Iteration 342, loss = 0.09159976\n","Iteration 343, loss = 0.09145034\n","Iteration 344, loss = 0.09129227\n","Iteration 345, loss = 0.09113838\n","Iteration 346, loss = 0.09098994\n","Iteration 347, loss = 0.09082496\n","Iteration 348, loss = 0.09068040\n","Iteration 349, loss = 0.09052160\n","Iteration 350, loss = 0.09036362\n","Iteration 351, loss = 0.09022477\n","Iteration 352, loss = 0.09009112\n","Iteration 353, loss = 0.08994777\n","Iteration 354, loss = 0.08980472\n","Iteration 355, loss = 0.08966300\n","Iteration 356, loss = 0.08949145\n","Iteration 357, loss = 0.08937003\n","Iteration 358, loss = 0.08922189\n","Iteration 359, loss = 0.08907439\n","Iteration 360, loss = 0.08896328\n","Iteration 361, loss = 0.08879225\n","Iteration 362, loss = 0.08866013\n","Iteration 363, loss = 0.08853440\n","Iteration 364, loss = 0.08837916\n","Iteration 365, loss = 0.08824611\n","Iteration 366, loss = 0.08810834\n","Iteration 367, loss = 0.08798706\n","Iteration 368, loss = 0.08784216\n","Iteration 369, loss = 0.08772009\n","Iteration 370, loss = 0.08759622\n","Iteration 371, loss = 0.08747228\n","Iteration 372, loss = 0.08733340\n","Iteration 373, loss = 0.08721346\n","Iteration 374, loss = 0.08707666\n","Iteration 375, loss = 0.08698585\n","Iteration 376, loss = 0.08684733\n","Iteration 377, loss = 0.08673836\n","Iteration 378, loss = 0.08662183\n","Iteration 379, loss = 0.08647921\n","Iteration 380, loss = 0.08635912\n","Iteration 381, loss = 0.08626708\n","Iteration 382, loss = 0.08611722\n","Iteration 383, loss = 0.08602696\n","Iteration 384, loss = 0.08590190\n","Iteration 385, loss = 0.08579140\n","Iteration 386, loss = 0.08569040\n","Iteration 387, loss = 0.08557489\n","Iteration 388, loss = 0.08546950\n","Iteration 389, loss = 0.08534501\n","Iteration 390, loss = 0.08522983\n","Iteration 391, loss = 0.08513088\n","Iteration 392, loss = 0.08502248\n","Iteration 393, loss = 0.08491151\n","Iteration 394, loss = 0.08479819\n","Iteration 395, loss = 0.08472189\n","Iteration 396, loss = 0.08458979\n","Iteration 397, loss = 0.08447974\n","Iteration 398, loss = 0.08435812\n","Iteration 399, loss = 0.08429656\n","Iteration 400, loss = 0.08416008\n","Iteration 401, loss = 0.08404969\n","Iteration 402, loss = 0.08394224\n","Iteration 403, loss = 0.08384141\n","Iteration 404, loss = 0.08372435\n","Iteration 405, loss = 0.08363099\n","Iteration 406, loss = 0.08351758\n","Iteration 407, loss = 0.08341778\n","Iteration 408, loss = 0.08329582\n","Iteration 409, loss = 0.08322971\n","Iteration 410, loss = 0.08311275\n","Iteration 411, loss = 0.08302109\n","Iteration 412, loss = 0.08289670\n","Iteration 413, loss = 0.08277716\n","Iteration 414, loss = 0.08268842\n","Iteration 415, loss = 0.08257853\n","Iteration 416, loss = 0.08245293\n","Iteration 417, loss = 0.08235479\n","Iteration 418, loss = 0.08225015\n","Iteration 419, loss = 0.08213841\n","Iteration 420, loss = 0.08204947\n","Iteration 421, loss = 0.08192417\n","Iteration 422, loss = 0.08182881\n","Iteration 423, loss = 0.08174036\n","Iteration 424, loss = 0.08161550\n","Iteration 425, loss = 0.08155760\n","Iteration 426, loss = 0.08141613\n","Iteration 427, loss = 0.08131857\n","Iteration 428, loss = 0.08120512\n","Iteration 429, loss = 0.08110049\n","Iteration 430, loss = 0.08101176\n","Iteration 431, loss = 0.08091271\n","Iteration 432, loss = 0.08082589\n","Iteration 433, loss = 0.08074792\n","Iteration 434, loss = 0.08063944\n","Iteration 435, loss = 0.08056128\n","Iteration 436, loss = 0.08044351\n","Iteration 437, loss = 0.08036494\n","Iteration 438, loss = 0.08029926\n","Iteration 439, loss = 0.08019737\n","Iteration 440, loss = 0.08007877\n","Iteration 441, loss = 0.08000719\n","Iteration 442, loss = 0.07988487\n","Iteration 443, loss = 0.07980150\n","Iteration 444, loss = 0.07972050\n","Iteration 445, loss = 0.07959409\n","Iteration 446, loss = 0.07952344\n","Iteration 447, loss = 0.07943324\n","Iteration 448, loss = 0.07933824\n","Iteration 449, loss = 0.07927129\n","Iteration 450, loss = 0.07916280\n","Iteration 451, loss = 0.07907473\n","Iteration 452, loss = 0.07903337\n","Iteration 453, loss = 0.07886449\n","Iteration 454, loss = 0.07877708\n","Iteration 455, loss = 0.07870098\n","Iteration 456, loss = 0.07860322\n","Iteration 457, loss = 0.07852554\n","Iteration 458, loss = 0.07842127\n","Iteration 459, loss = 0.07834645\n","Iteration 460, loss = 0.07824645\n","Iteration 461, loss = 0.07819130\n","Iteration 462, loss = 0.07806893\n","Iteration 463, loss = 0.07800304\n","Iteration 464, loss = 0.07791331\n","Iteration 465, loss = 0.07782109\n","Iteration 466, loss = 0.07773549\n","Iteration 467, loss = 0.07763094\n","Iteration 468, loss = 0.07756491\n","Iteration 469, loss = 0.07747589\n","Iteration 470, loss = 0.07739448\n","Iteration 471, loss = 0.07733908\n","Iteration 472, loss = 0.07722541\n","Iteration 473, loss = 0.07713802\n","Iteration 474, loss = 0.07707705\n","Iteration 475, loss = 0.07696990\n","Iteration 476, loss = 0.07689949\n","Iteration 477, loss = 0.07683133\n","Iteration 478, loss = 0.07673041\n","Iteration 479, loss = 0.07666497\n","Iteration 480, loss = 0.07656364\n","Iteration 481, loss = 0.07647239\n","Iteration 482, loss = 0.07639332\n","Iteration 483, loss = 0.07632064\n","Iteration 484, loss = 0.07624037\n","Iteration 485, loss = 0.07612346\n","Iteration 486, loss = 0.07602795\n","Iteration 487, loss = 0.07595848\n","Iteration 488, loss = 0.07586451\n","Iteration 489, loss = 0.07581223\n","Iteration 490, loss = 0.07570789\n","Iteration 491, loss = 0.07563273\n","Iteration 492, loss = 0.07554017\n","Iteration 493, loss = 0.07546156\n","Iteration 494, loss = 0.07541599\n","Iteration 495, loss = 0.07529613\n","Iteration 496, loss = 0.07520949\n","Iteration 497, loss = 0.07516509\n","Iteration 498, loss = 0.07505603\n","Iteration 499, loss = 0.07501574\n","Iteration 500, loss = 0.07490199\n","Iteration 501, loss = 0.07482645\n","Iteration 502, loss = 0.07472618\n","Iteration 503, loss = 0.07466433\n","Iteration 504, loss = 0.07456497\n","Iteration 505, loss = 0.07450068\n","Iteration 506, loss = 0.07441492\n","Iteration 507, loss = 0.07437955\n","Iteration 508, loss = 0.07427772\n","Iteration 509, loss = 0.07420140\n","Iteration 510, loss = 0.07410128\n","Iteration 511, loss = 0.07403371\n","Iteration 512, loss = 0.07394893\n","Iteration 513, loss = 0.07387400\n","Iteration 514, loss = 0.07379552\n","Iteration 515, loss = 0.07371712\n","Iteration 516, loss = 0.07365358\n","Iteration 517, loss = 0.07358671\n","Iteration 518, loss = 0.07349472\n","Iteration 519, loss = 0.07344447\n","Iteration 520, loss = 0.07335169\n","Iteration 521, loss = 0.07327606\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n","       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n","       hidden_layer_sizes=(20, 20, 5), learning_rate='constant',\n","       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n","       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n","       random_state=0, shuffle=True, solver='sgd', tol=0.0001,\n","       validation_fraction=0.1, verbose=10, warm_start=False)"]},"metadata":{"tags":[]},"execution_count":46}]},{"metadata":{"id":"CyQ-lLF4esid","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"a67a5342-9c22-4b4a-db79-ececfa6e7700","executionInfo":{"status":"ok","timestamp":1540046018988,"user_tz":-480,"elapsed":1043,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# performance on validation data\n","print('Results on validation set...')\n","y_pred = mlp.predict(X_val)\n","confusion = confusion_matrix(y_val, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_val, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_val, y_pred, 2))"],"execution_count":47,"outputs":[{"output_type":"stream","text":["Results on validation set...\n","[[21880 35023]\n"," [    2    55]]\n","Classification accuracy: 0.3850948033707865\n","Sensitivity: 0.9649122807017544\n","Precision: 0.0015679343178060324\n","F2 score: 0.007789044355067128\n"],"name":"stdout"}]},{"metadata":{"id":"kW2CjhvvesmQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":9010},"outputId":"6067a51a-4984-492f-efdb-32c2658b9a3e","executionInfo":{"status":"ok","timestamp":1540046085300,"user_tz":-480,"elapsed":2521,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# l2 regularisation\n","mlp = MLPClassifier(solver='sgd', # stochastic gradient descent\n","                    alpha=0.0002, # regularisation\n","                    hidden_layer_sizes=(20,20,5), verbose=10, # state log\n","                    random_state=0, max_iter=1000, tol=1e-4) # set tolerance as 1e-4\n","mlp.fit(X_nmtrain, y_nmtrain)"],"execution_count":54,"outputs":[{"output_type":"stream","text":["Iteration 1, loss = 0.77467588\n","Iteration 2, loss = 0.70152687\n","Iteration 3, loss = 0.63283945\n","Iteration 4, loss = 0.57583728\n","Iteration 5, loss = 0.52600449\n","Iteration 6, loss = 0.48642923\n","Iteration 7, loss = 0.45436241\n","Iteration 8, loss = 0.43208479\n","Iteration 9, loss = 0.41716858\n","Iteration 10, loss = 0.40629545\n","Iteration 11, loss = 0.39802361\n","Iteration 12, loss = 0.39226891\n","Iteration 13, loss = 0.38722717\n","Iteration 14, loss = 0.38319440\n","Iteration 15, loss = 0.37978517\n","Iteration 16, loss = 0.37666375\n","Iteration 17, loss = 0.37384868\n","Iteration 18, loss = 0.37131171\n","Iteration 19, loss = 0.36873736\n","Iteration 20, loss = 0.36641294\n","Iteration 21, loss = 0.36404250\n","Iteration 22, loss = 0.36205810\n","Iteration 23, loss = 0.35996498\n","Iteration 24, loss = 0.35793128\n","Iteration 25, loss = 0.35594260\n","Iteration 26, loss = 0.35403799\n","Iteration 27, loss = 0.35226726\n","Iteration 28, loss = 0.35032047\n","Iteration 29, loss = 0.34848243\n","Iteration 30, loss = 0.34672411\n","Iteration 31, loss = 0.34490348\n","Iteration 32, loss = 0.34310876\n","Iteration 33, loss = 0.34141493\n","Iteration 34, loss = 0.33963328\n","Iteration 35, loss = 0.33791502\n","Iteration 36, loss = 0.33617235\n","Iteration 37, loss = 0.33440883\n","Iteration 38, loss = 0.33265286\n","Iteration 39, loss = 0.33084484\n","Iteration 40, loss = 0.32897351\n","Iteration 41, loss = 0.32719054\n","Iteration 42, loss = 0.32516320\n","Iteration 43, loss = 0.32325446\n","Iteration 44, loss = 0.32130041\n","Iteration 45, loss = 0.31926381\n","Iteration 46, loss = 0.31726804\n","Iteration 47, loss = 0.31514006\n","Iteration 48, loss = 0.31298210\n","Iteration 49, loss = 0.31082891\n","Iteration 50, loss = 0.30857237\n","Iteration 51, loss = 0.30638275\n","Iteration 52, loss = 0.30430049\n","Iteration 53, loss = 0.30203337\n","Iteration 54, loss = 0.29979136\n","Iteration 55, loss = 0.29758282\n","Iteration 56, loss = 0.29534522\n","Iteration 57, loss = 0.29306590\n","Iteration 58, loss = 0.29083042\n","Iteration 59, loss = 0.28855621\n","Iteration 60, loss = 0.28629633\n","Iteration 61, loss = 0.28417547\n","Iteration 62, loss = 0.28196912\n","Iteration 63, loss = 0.27989436\n","Iteration 64, loss = 0.27783500\n","Iteration 65, loss = 0.27582696\n","Iteration 66, loss = 0.27390179\n","Iteration 67, loss = 0.27190400\n","Iteration 68, loss = 0.27005169\n","Iteration 69, loss = 0.26828247\n","Iteration 70, loss = 0.26643666\n","Iteration 71, loss = 0.26459959\n","Iteration 72, loss = 0.26286603\n","Iteration 73, loss = 0.26114856\n","Iteration 74, loss = 0.25940400\n","Iteration 75, loss = 0.25773249\n","Iteration 76, loss = 0.25610274\n","Iteration 77, loss = 0.25448181\n","Iteration 78, loss = 0.25287025\n","Iteration 79, loss = 0.25128063\n","Iteration 80, loss = 0.24975384\n","Iteration 81, loss = 0.24823773\n","Iteration 82, loss = 0.24671290\n","Iteration 83, loss = 0.24521707\n","Iteration 84, loss = 0.24379696\n","Iteration 85, loss = 0.24234848\n","Iteration 86, loss = 0.24093388\n","Iteration 87, loss = 0.23957312\n","Iteration 88, loss = 0.23832726\n","Iteration 89, loss = 0.23689644\n","Iteration 90, loss = 0.23555182\n","Iteration 91, loss = 0.23425255\n","Iteration 92, loss = 0.23299278\n","Iteration 93, loss = 0.23174034\n","Iteration 94, loss = 0.23051137\n","Iteration 95, loss = 0.22925363\n","Iteration 96, loss = 0.22805005\n","Iteration 97, loss = 0.22685981\n","Iteration 98, loss = 0.22567240\n","Iteration 99, loss = 0.22449436\n","Iteration 100, loss = 0.22330142\n","Iteration 101, loss = 0.22220868\n","Iteration 102, loss = 0.22107633\n","Iteration 103, loss = 0.21994057\n","Iteration 104, loss = 0.21882663\n","Iteration 105, loss = 0.21770780\n","Iteration 106, loss = 0.21657147\n","Iteration 107, loss = 0.21548016\n","Iteration 108, loss = 0.21439629\n","Iteration 109, loss = 0.21326090\n","Iteration 110, loss = 0.21222095\n","Iteration 111, loss = 0.21110325\n","Iteration 112, loss = 0.21002997\n","Iteration 113, loss = 0.20893726\n","Iteration 114, loss = 0.20785987\n","Iteration 115, loss = 0.20678958\n","Iteration 116, loss = 0.20570517\n","Iteration 117, loss = 0.20460704\n","Iteration 118, loss = 0.20354347\n","Iteration 119, loss = 0.20246670\n","Iteration 120, loss = 0.20141548\n","Iteration 121, loss = 0.20031858\n","Iteration 122, loss = 0.19916694\n","Iteration 123, loss = 0.19814828\n","Iteration 124, loss = 0.19708242\n","Iteration 125, loss = 0.19609105\n","Iteration 126, loss = 0.19497575\n","Iteration 127, loss = 0.19394882\n","Iteration 128, loss = 0.19284928\n","Iteration 129, loss = 0.19175851\n","Iteration 130, loss = 0.19075864\n","Iteration 131, loss = 0.18972825\n","Iteration 132, loss = 0.18867397\n","Iteration 133, loss = 0.18768517\n","Iteration 134, loss = 0.18663116\n","Iteration 135, loss = 0.18565752\n","Iteration 136, loss = 0.18467251\n","Iteration 137, loss = 0.18364428\n","Iteration 138, loss = 0.18266591\n","Iteration 139, loss = 0.18169763\n","Iteration 140, loss = 0.18071466\n","Iteration 141, loss = 0.17975524\n","Iteration 142, loss = 0.17880929\n","Iteration 143, loss = 0.17784976\n","Iteration 144, loss = 0.17686921\n","Iteration 145, loss = 0.17597389\n","Iteration 146, loss = 0.17501493\n","Iteration 147, loss = 0.17416314\n","Iteration 148, loss = 0.17316295\n","Iteration 149, loss = 0.17224885\n","Iteration 150, loss = 0.17136174\n","Iteration 151, loss = 0.17042818\n","Iteration 152, loss = 0.16952902\n","Iteration 153, loss = 0.16864977\n","Iteration 154, loss = 0.16776852\n","Iteration 155, loss = 0.16689858\n","Iteration 156, loss = 0.16608720\n","Iteration 157, loss = 0.16519829\n","Iteration 158, loss = 0.16432662\n","Iteration 159, loss = 0.16355341\n","Iteration 160, loss = 0.16262719\n","Iteration 161, loss = 0.16182747\n","Iteration 162, loss = 0.16102232\n","Iteration 163, loss = 0.16023550\n","Iteration 164, loss = 0.15937937\n","Iteration 165, loss = 0.15861921\n","Iteration 166, loss = 0.15777117\n","Iteration 167, loss = 0.15703503\n","Iteration 168, loss = 0.15627246\n","Iteration 169, loss = 0.15550562\n","Iteration 170, loss = 0.15471261\n","Iteration 171, loss = 0.15394308\n","Iteration 172, loss = 0.15325989\n","Iteration 173, loss = 0.15245860\n","Iteration 174, loss = 0.15167427\n","Iteration 175, loss = 0.15094469\n","Iteration 176, loss = 0.15023277\n","Iteration 177, loss = 0.14951342\n","Iteration 178, loss = 0.14885999\n","Iteration 179, loss = 0.14807898\n","Iteration 180, loss = 0.14739872\n","Iteration 181, loss = 0.14674596\n","Iteration 182, loss = 0.14601025\n","Iteration 183, loss = 0.14539240\n","Iteration 184, loss = 0.14469664\n","Iteration 185, loss = 0.14405110\n","Iteration 186, loss = 0.14341223\n","Iteration 187, loss = 0.14272456\n","Iteration 188, loss = 0.14209572\n","Iteration 189, loss = 0.14149267\n","Iteration 190, loss = 0.14087169\n","Iteration 191, loss = 0.14025445\n","Iteration 192, loss = 0.13964993\n","Iteration 193, loss = 0.13905873\n","Iteration 194, loss = 0.13845214\n","Iteration 195, loss = 0.13789866\n","Iteration 196, loss = 0.13730832\n","Iteration 197, loss = 0.13673747\n","Iteration 198, loss = 0.13618854\n","Iteration 199, loss = 0.13560744\n","Iteration 200, loss = 0.13505231\n","Iteration 201, loss = 0.13453926\n","Iteration 202, loss = 0.13399090\n","Iteration 203, loss = 0.13345776\n","Iteration 204, loss = 0.13294217\n","Iteration 205, loss = 0.13242519\n","Iteration 206, loss = 0.13186956\n","Iteration 207, loss = 0.13138754\n","Iteration 208, loss = 0.13085736\n","Iteration 209, loss = 0.13039266\n","Iteration 210, loss = 0.12989082\n","Iteration 211, loss = 0.12935626\n","Iteration 212, loss = 0.12890314\n","Iteration 213, loss = 0.12842181\n","Iteration 214, loss = 0.12790246\n","Iteration 215, loss = 0.12743821\n","Iteration 216, loss = 0.12697221\n","Iteration 217, loss = 0.12646166\n","Iteration 218, loss = 0.12602034\n","Iteration 219, loss = 0.12553144\n","Iteration 220, loss = 0.12502323\n","Iteration 221, loss = 0.12457765\n","Iteration 222, loss = 0.12411246\n","Iteration 223, loss = 0.12359173\n","Iteration 224, loss = 0.12316496\n","Iteration 225, loss = 0.12270363\n","Iteration 226, loss = 0.12222098\n","Iteration 227, loss = 0.12176727\n","Iteration 228, loss = 0.12128620\n","Iteration 229, loss = 0.12087252\n","Iteration 230, loss = 0.12041461\n","Iteration 231, loss = 0.12002913\n","Iteration 232, loss = 0.11962710\n","Iteration 233, loss = 0.11924799\n","Iteration 234, loss = 0.11887423\n","Iteration 235, loss = 0.11848385\n","Iteration 236, loss = 0.11814499\n","Iteration 237, loss = 0.11775967\n","Iteration 238, loss = 0.11742195\n","Iteration 239, loss = 0.11706392\n","Iteration 240, loss = 0.11668646\n","Iteration 241, loss = 0.11635946\n","Iteration 242, loss = 0.11599567\n","Iteration 243, loss = 0.11565317\n","Iteration 244, loss = 0.11531477\n","Iteration 245, loss = 0.11500638\n","Iteration 246, loss = 0.11469056\n","Iteration 247, loss = 0.11435884\n","Iteration 248, loss = 0.11403852\n","Iteration 249, loss = 0.11371839\n","Iteration 250, loss = 0.11341312\n","Iteration 251, loss = 0.11311637\n","Iteration 252, loss = 0.11280701\n","Iteration 253, loss = 0.11252651\n","Iteration 254, loss = 0.11216335\n","Iteration 255, loss = 0.11188947\n","Iteration 256, loss = 0.11158482\n","Iteration 257, loss = 0.11127282\n","Iteration 258, loss = 0.11099254\n","Iteration 259, loss = 0.11069582\n","Iteration 260, loss = 0.11036705\n","Iteration 261, loss = 0.11009163\n","Iteration 262, loss = 0.10981915\n","Iteration 263, loss = 0.10951258\n","Iteration 264, loss = 0.10924415\n","Iteration 265, loss = 0.10894196\n","Iteration 266, loss = 0.10868468\n","Iteration 267, loss = 0.10841246\n","Iteration 268, loss = 0.10814337\n","Iteration 269, loss = 0.10785147\n","Iteration 270, loss = 0.10759861\n","Iteration 271, loss = 0.10732954\n","Iteration 272, loss = 0.10707065\n","Iteration 273, loss = 0.10680338\n","Iteration 274, loss = 0.10654632\n","Iteration 275, loss = 0.10629473\n","Iteration 276, loss = 0.10604038\n","Iteration 277, loss = 0.10575919\n","Iteration 278, loss = 0.10553579\n","Iteration 279, loss = 0.10527649\n","Iteration 280, loss = 0.10503316\n","Iteration 281, loss = 0.10478487\n","Iteration 282, loss = 0.10456373\n","Iteration 283, loss = 0.10430034\n","Iteration 284, loss = 0.10399954\n","Iteration 285, loss = 0.10374572\n","Iteration 286, loss = 0.10350987\n","Iteration 287, loss = 0.10324774\n","Iteration 288, loss = 0.10299221\n","Iteration 289, loss = 0.10275314\n","Iteration 290, loss = 0.10252419\n","Iteration 291, loss = 0.10227151\n","Iteration 292, loss = 0.10202665\n","Iteration 293, loss = 0.10180295\n","Iteration 294, loss = 0.10156150\n","Iteration 295, loss = 0.10132109\n","Iteration 296, loss = 0.10110619\n","Iteration 297, loss = 0.10091111\n","Iteration 298, loss = 0.10062998\n","Iteration 299, loss = 0.10041710\n","Iteration 300, loss = 0.10018072\n","Iteration 301, loss = 0.09998875\n","Iteration 302, loss = 0.09976197\n","Iteration 303, loss = 0.09957398\n","Iteration 304, loss = 0.09933833\n","Iteration 305, loss = 0.09910911\n","Iteration 306, loss = 0.09888845\n","Iteration 307, loss = 0.09868441\n","Iteration 308, loss = 0.09843017\n","Iteration 309, loss = 0.09822753\n","Iteration 310, loss = 0.09803887\n","Iteration 311, loss = 0.09778679\n","Iteration 312, loss = 0.09759756\n","Iteration 313, loss = 0.09737807\n","Iteration 314, loss = 0.09715634\n","Iteration 315, loss = 0.09696832\n","Iteration 316, loss = 0.09676511\n","Iteration 317, loss = 0.09654643\n","Iteration 318, loss = 0.09633586\n","Iteration 319, loss = 0.09614332\n","Iteration 320, loss = 0.09592567\n","Iteration 321, loss = 0.09572547\n","Iteration 322, loss = 0.09552434\n","Iteration 323, loss = 0.09533256\n","Iteration 324, loss = 0.09515019\n","Iteration 325, loss = 0.09494424\n","Iteration 326, loss = 0.09474100\n","Iteration 327, loss = 0.09453759\n","Iteration 328, loss = 0.09435928\n","Iteration 329, loss = 0.09412635\n","Iteration 330, loss = 0.09393992\n","Iteration 331, loss = 0.09374540\n","Iteration 332, loss = 0.09351708\n","Iteration 333, loss = 0.09333928\n","Iteration 334, loss = 0.09313762\n","Iteration 335, loss = 0.09294450\n","Iteration 336, loss = 0.09270437\n","Iteration 337, loss = 0.09252560\n","Iteration 338, loss = 0.09233856\n","Iteration 339, loss = 0.09213334\n","Iteration 340, loss = 0.09195335\n","Iteration 341, loss = 0.09179178\n","Iteration 342, loss = 0.09161749\n","Iteration 343, loss = 0.09146812\n","Iteration 344, loss = 0.09131016\n","Iteration 345, loss = 0.09115607\n","Iteration 346, loss = 0.09100774\n","Iteration 347, loss = 0.09084268\n","Iteration 348, loss = 0.09069814\n","Iteration 349, loss = 0.09053940\n","Iteration 350, loss = 0.09038134\n","Iteration 351, loss = 0.09024248\n","Iteration 352, loss = 0.09010872\n","Iteration 353, loss = 0.08996562\n","Iteration 354, loss = 0.08982229\n","Iteration 355, loss = 0.08968079\n","Iteration 356, loss = 0.08950910\n","Iteration 357, loss = 0.08938768\n","Iteration 358, loss = 0.08923940\n","Iteration 359, loss = 0.08909189\n","Iteration 360, loss = 0.08898078\n","Iteration 361, loss = 0.08881000\n","Iteration 362, loss = 0.08867755\n","Iteration 363, loss = 0.08855212\n","Iteration 364, loss = 0.08839696\n","Iteration 365, loss = 0.08826361\n","Iteration 366, loss = 0.08812610\n","Iteration 367, loss = 0.08800493\n","Iteration 368, loss = 0.08786010\n","Iteration 369, loss = 0.08773788\n","Iteration 370, loss = 0.08761398\n","Iteration 371, loss = 0.08749009\n","Iteration 372, loss = 0.08735119\n","Iteration 373, loss = 0.08723137\n","Iteration 374, loss = 0.08709441\n","Iteration 375, loss = 0.08700370\n","Iteration 376, loss = 0.08686512\n","Iteration 377, loss = 0.08675631\n","Iteration 378, loss = 0.08663959\n","Iteration 379, loss = 0.08649679\n","Iteration 380, loss = 0.08637701\n","Iteration 381, loss = 0.08628515\n","Iteration 382, loss = 0.08613491\n","Iteration 383, loss = 0.08604520\n","Iteration 384, loss = 0.08591988\n","Iteration 385, loss = 0.08580921\n","Iteration 386, loss = 0.08570811\n","Iteration 387, loss = 0.08559278\n","Iteration 388, loss = 0.08548757\n","Iteration 389, loss = 0.08536286\n","Iteration 390, loss = 0.08524772\n","Iteration 391, loss = 0.08514858\n","Iteration 392, loss = 0.08504012\n","Iteration 393, loss = 0.08492937\n","Iteration 394, loss = 0.08481613\n","Iteration 395, loss = 0.08473952\n","Iteration 396, loss = 0.08460728\n","Iteration 397, loss = 0.08449755\n","Iteration 398, loss = 0.08437591\n","Iteration 399, loss = 0.08431421\n","Iteration 400, loss = 0.08417772\n","Iteration 401, loss = 0.08406737\n","Iteration 402, loss = 0.08395987\n","Iteration 403, loss = 0.08385902\n","Iteration 404, loss = 0.08374184\n","Iteration 405, loss = 0.08364875\n","Iteration 406, loss = 0.08353552\n","Iteration 407, loss = 0.08343551\n","Iteration 408, loss = 0.08331350\n","Iteration 409, loss = 0.08324735\n","Iteration 410, loss = 0.08313008\n","Iteration 411, loss = 0.08303879\n","Iteration 412, loss = 0.08291459\n","Iteration 413, loss = 0.08279499\n","Iteration 414, loss = 0.08270595\n","Iteration 415, loss = 0.08259640\n","Iteration 416, loss = 0.08247093\n","Iteration 417, loss = 0.08237296\n","Iteration 418, loss = 0.08226808\n","Iteration 419, loss = 0.08215579\n","Iteration 420, loss = 0.08206747\n","Iteration 421, loss = 0.08194214\n","Iteration 422, loss = 0.08184671\n","Iteration 423, loss = 0.08175813\n","Iteration 424, loss = 0.08163326\n","Iteration 425, loss = 0.08157527\n","Iteration 426, loss = 0.08143391\n","Iteration 427, loss = 0.08133632\n","Iteration 428, loss = 0.08122295\n","Iteration 429, loss = 0.08111803\n","Iteration 430, loss = 0.08102972\n","Iteration 431, loss = 0.08093075\n","Iteration 432, loss = 0.08084315\n","Iteration 433, loss = 0.08076556\n","Iteration 434, loss = 0.08065719\n","Iteration 435, loss = 0.08057905\n","Iteration 436, loss = 0.08046142\n","Iteration 437, loss = 0.08038252\n","Iteration 438, loss = 0.08031734\n","Iteration 439, loss = 0.08021535\n","Iteration 440, loss = 0.08009663\n","Iteration 441, loss = 0.08002510\n","Iteration 442, loss = 0.07990281\n","Iteration 443, loss = 0.07981928\n","Iteration 444, loss = 0.07973815\n","Iteration 445, loss = 0.07961193\n","Iteration 446, loss = 0.07954108\n","Iteration 447, loss = 0.07945088\n","Iteration 448, loss = 0.07935591\n","Iteration 449, loss = 0.07928896\n","Iteration 450, loss = 0.07918040\n","Iteration 451, loss = 0.07909293\n","Iteration 452, loss = 0.07905144\n","Iteration 453, loss = 0.07888247\n","Iteration 454, loss = 0.07879476\n","Iteration 455, loss = 0.07871921\n","Iteration 456, loss = 0.07862008\n","Iteration 457, loss = 0.07854474\n","Iteration 458, loss = 0.07844347\n","Iteration 459, loss = 0.07837007\n","Iteration 460, loss = 0.07826782\n","Iteration 461, loss = 0.07821357\n","Iteration 462, loss = 0.07809195\n","Iteration 463, loss = 0.07801452\n","Iteration 464, loss = 0.07793538\n","Iteration 465, loss = 0.07784055\n","Iteration 466, loss = 0.07775900\n","Iteration 467, loss = 0.07764979\n","Iteration 468, loss = 0.07757912\n","Iteration 469, loss = 0.07749048\n","Iteration 470, loss = 0.07740721\n","Iteration 471, loss = 0.07735077\n","Iteration 472, loss = 0.07723770\n","Iteration 473, loss = 0.07714924\n","Iteration 474, loss = 0.07708816\n","Iteration 475, loss = 0.07698263\n","Iteration 476, loss = 0.07691099\n","Iteration 477, loss = 0.07684341\n","Iteration 478, loss = 0.07674498\n","Iteration 479, loss = 0.07668066\n","Iteration 480, loss = 0.07657857\n","Iteration 481, loss = 0.07650381\n","Iteration 482, loss = 0.07641723\n","Iteration 483, loss = 0.07634467\n","Iteration 484, loss = 0.07626582\n","Iteration 485, loss = 0.07614455\n","Iteration 486, loss = 0.07604851\n","Iteration 487, loss = 0.07597800\n","Iteration 488, loss = 0.07588446\n","Iteration 489, loss = 0.07582888\n","Iteration 490, loss = 0.07572379\n","Iteration 491, loss = 0.07564436\n","Iteration 492, loss = 0.07555546\n","Iteration 493, loss = 0.07548036\n","Iteration 494, loss = 0.07543419\n","Iteration 495, loss = 0.07531558\n","Iteration 496, loss = 0.07522659\n","Iteration 497, loss = 0.07517821\n","Iteration 498, loss = 0.07507385\n","Iteration 499, loss = 0.07503154\n","Iteration 500, loss = 0.07491964\n","Iteration 501, loss = 0.07484423\n","Iteration 502, loss = 0.07474372\n","Iteration 503, loss = 0.07468058\n","Iteration 504, loss = 0.07458943\n","Iteration 505, loss = 0.07451822\n","Iteration 506, loss = 0.07443190\n","Iteration 507, loss = 0.07439704\n","Iteration 508, loss = 0.07429531\n","Iteration 509, loss = 0.07421894\n","Iteration 510, loss = 0.07411850\n","Iteration 511, loss = 0.07405228\n","Iteration 512, loss = 0.07396671\n","Iteration 513, loss = 0.07389154\n","Iteration 514, loss = 0.07382007\n","Iteration 515, loss = 0.07373659\n","Iteration 516, loss = 0.07366662\n","Iteration 517, loss = 0.07360046\n","Iteration 518, loss = 0.07350835\n","Iteration 519, loss = 0.07345821\n","Iteration 520, loss = 0.07336013\n","Iteration 521, loss = 0.07329514\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["MLPClassifier(activation='relu', alpha=0.0002, batch_size='auto', beta_1=0.9,\n","       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n","       hidden_layer_sizes=(20, 20, 5), learning_rate='constant',\n","       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n","       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n","       random_state=0, shuffle=True, solver='sgd', tol=0.0001,\n","       validation_fraction=0.1, verbose=10, warm_start=False)"]},"metadata":{"tags":[]},"execution_count":54}]},{"metadata":{"id":"plNLjhxBesgX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"6c4b509c-f4e7-45b9-bce9-416619792eb2","executionInfo":{"status":"ok","timestamp":1540046087857,"user_tz":-480,"elapsed":1016,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# performance on validation data\n","print('Results on validation set...')\n","y_pred = mlp.predict(X_val)\n","confusion = confusion_matrix(y_val, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_val, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_val, y_pred, 2))"],"execution_count":55,"outputs":[{"output_type":"stream","text":["Results on validation set...\n","[[21877 35026]\n"," [    2    55]]\n","Classification accuracy: 0.38504213483146066\n","Sensitivity: 0.9649122807017544\n","Precision: 0.001567800233744762\n","F2 score: 0.007788382565351611\n"],"name":"stdout"}]},{"metadata":{"id":"pYd-47VJeseJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"332a8528-de1c-46fd-99f0-fa52fb700132","executionInfo":{"status":"ok","timestamp":1540046092648,"user_tz":-480,"elapsed":1069,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["# performance on test data\n","print('Results on test set...')\n","y_pred = mlp.predict(X_test)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":56,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[23714 33174]\n"," [    3    72]]\n","Classification accuracy: 0.41756929936976633\n","Sensitivity: 0.96\n","Precision: 0.002165674066053059\n","F2 score: 0.010731532820604543\n"],"name":"stdout"}]},{"metadata":{"id":"ySSwYBaloqzv","colab_type":"text"},"cell_type":"markdown","source":["### 6) Explore stacked models"]},{"metadata":{"id":"nq-sYzNb-OwA","colab_type":"text"},"cell_type":"markdown","source":["#### 6a) K nearest neighbours & Random forest"]},{"metadata":{"id":"l4UavFQ_j_Pl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"a5d8b039-0ad5-40fd-eb89-7541362760ee","executionInfo":{"status":"ok","timestamp":1541827319838,"user_tz":-480,"elapsed":639002,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neural_network import MLPClassifier\n","\n","# split data equally into training and validation\n","X_stacktrain, X_stackval, y_stacktrain, y_stackval = train_test_split(X_smtrain, y_smtrain, test_size=0.5, \n","                                                                      shuffle=False, stratify=None)\n","\n","# train base learners with the same training set\n","knn = KNeighborsClassifier(n_neighbors=2)\n","rf = RandomForestClassifier(random_state=0, n_estimators=8)\n","knn.fit(X_stacktrain, y_stacktrain)\n","rf.fit(X_stacktrain, y_stacktrain)\n","\n","# use trained base learners to predict the classes in the validation set\n","knn_val_preds = knn.predict(X_stackval)\n","rf_val_preds = rf.predict(X_stackval)\n","stack_val_preds = pd.DataFrame(X_stackval.copy())\n","stack_val_preds['knn'] = knn_val_preds\n","stack_val_preds['rf'] = rf_val_preds\n","\n","# train meta learner on stacked predictions\n","meta_model = MLPClassifier(solver='sgd', alpha=0.0003,\n","                    hidden_layer_sizes=(20,20,5),\n","                    random_state=0, max_iter=1000)\n","meta_model.fit(stack_val_preds, y_stackval)\n","\n","# re-train base learners with full training set\n","knn.fit(X_smtrain, y_smtrain)\n","rf.fit(X_smtrain, y_smtrain)\n","\n","# use trained base leaners to predict the classes in the test set\n","knn_test_preds = knn.predict(X_test)\n","rf_test_preds = rf.predict(X_test)\n","stack_test_preds = pd.DataFrame(X_test.copy())\n","stack_test_preds['knn'] = knn_test_preds\n","stack_test_preds['rf'] = rf_test_preds\n","\n","# use trained meta learner to predict the classes in the test set\n","print('Results on test set...')\n","y_pred = meta_model.predict(stack_test_preds)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[56873    15]\n"," [   18    57]]\n","Classification accuracy: 0.999420676579534\n","Sensitivity: 0.76\n","Precision: 0.7916666666666666\n","F2 score: 0.7661290322580645\n"],"name":"stdout"}]},{"metadata":{"id":"36RqsHC1-W2-","colab_type":"text"},"cell_type":"markdown","source":["#### 6b) K nearest neighbours & Multi layer perceptron"]},{"metadata":{"id":"M9_PLnI7E5we","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"54920ce1-20cf-4545-ba39-86add00f08c3","executionInfo":{"status":"ok","timestamp":1541828720165,"user_tz":-480,"elapsed":745022,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neural_network import MLPClassifier\n","\n","# split data equally into training and validation\n","X_stacktrain, X_stackval, y_stacktrain, y_stackval = train_test_split(X_smtrain, y_smtrain, test_size=0.5, \n","                                                                     shuffle=False, stratify=None)\n","\n","# train base learners with the same training set\n","knn = KNeighborsClassifier(n_neighbors=2)\n","mlp = MLPClassifier(solver='sgd', alpha=0.0003,\n","                    hidden_layer_sizes=(20,20,5),\n","                    random_state=0, max_iter=1000)\n","knn.fit(X_stacktrain, y_stacktrain)\n","mlp.fit(X_stacktrain, y_stacktrain)\n","\n","# use trained base learners to predict the classes in the validation set\n","knn_val_preds = knn.predict(X_stackval)\n","mlp_val_preds = rf.predict(X_stackval)\n","stack_val_preds = pd.DataFrame(X_stackval.copy())\n","stack_val_preds['knn'] = knn_val_preds\n","stack_val_preds['mlp'] = mlp_val_preds\n","\n","# train meta learner on stacked predictions\n","meta_model = RandomForestClassifier(random_state=0, n_estimators=8)\n","meta_model.fit(stack_val_preds, y_stackval)\n","\n","# re-train base learners with full training set\n","knn.fit(X_smtrain, y_smtrain)\n","mlp.fit(X_smtrain, y_smtrain)\n","\n","# use trained base leaners to predict the classes in the test set\n","knn_test_preds = knn.predict(X_test)\n","mlp_test_preds = mlp.predict(X_test)\n","stack_test_preds = pd.DataFrame(X_test.copy())\n","stack_test_preds['knn'] = knn_test_preds\n","stack_test_preds['mlp'] = mlp_test_preds\n","\n","# use trained meta learner to predict the classes in the test set\n","print('Results on test set...')\n","y_pred = meta_model.predict(stack_test_preds)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":37,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[56882     6]\n"," [   19    56]]\n","Classification accuracy: 0.9995611186208592\n","Sensitivity: 0.7466666666666667\n","Precision: 0.9032258064516129\n","F2 score: 0.7734806629834254\n"],"name":"stdout"}]},{"metadata":{"id":"V6ykzrVg-XYF","colab_type":"text"},"cell_type":"markdown","source":["#### 6c) Random forest & Multi layer perceptron"]},{"metadata":{"id":"F9xfPfc_GE2h","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"f723e27c-f95e-4b0f-809a-f024561d2a32","executionInfo":{"status":"ok","timestamp":1541833729708,"user_tz":-480,"elapsed":594489,"user":{"displayName":"Dexter Wah","photoUrl":"","userId":"01507089259474555070"}}},"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neural_network import MLPClassifier\n","\n","# split data equally into training and validation\n","X_stacktrain, X_stackval, y_stacktrain, y_stackval = train_test_split(X_smtrain, y_smtrain, test_size=0.5, \n","                                                                     shuffle=False, stratify=None)\n","\n","# train base learners with the same training set\n","rf = RandomForestClassifier(random_state=0, n_estimators=8)\n","mlp = MLPClassifier(solver='sgd', alpha=0.0003,\n","                    hidden_layer_sizes=(20,20,5),\n","                    random_state=0, max_iter=1000)\n","rf.fit(X_stacktrain, y_stacktrain)\n","mlp.fit(X_stacktrain, y_stacktrain)\n","\n","# use trained base learners to predict the classes in the validation set\n","rf_val_preds = knn.predict(X_stackval)\n","mlp_val_preds = rf.predict(X_stackval)\n","stack_val_preds = pd.DataFrame(X_stackval.copy())\n","stack_val_preds['rf'] = rf_val_preds\n","stack_val_preds['mlp'] = mlp_val_preds\n","\n","# train meta learner on stacked predictions\n","meta_model = KNeighborsClassifier(n_neighbors=2)\n","meta_model.fit(stack_val_preds, y_stackval)\n","\n","# re-train base learners with full training set\n","rf.fit(X_smtrain, y_smtrain)\n","mlp.fit(X_smtrain, y_smtrain)\n","\n","# use trained base leaners to predict the classes in the test set\n","rf_test_preds = rf.predict(X_test)\n","mlp_test_preds = mlp.predict(X_test)\n","stack_test_preds = pd.DataFrame(X_test.copy())\n","stack_test_preds['rf'] = rf_test_preds\n","stack_test_preds['mlp'] = mlp_test_preds\n","\n","# use trained meta learner to predict the classes in the test set\n","print('Results on test set...')\n","y_pred = meta_model.predict(stack_test_preds)\n","confusion = confusion_matrix(y_test, y_pred)\n","TP = confusion[1][1]\n","TN = confusion[0][0]\n","FP = confusion[0][1]\n","FN = confusion[1][0]\n","sensitivity = TP / float(TP + FN)\n","precision = TP / float(TP + FP)\n","print(confusion)\n","print('Classification accuracy:', accuracy_score(y_test, y_pred))\n","print('Sensitivity:', sensitivity)\n","print('Precision:', precision)\n","print('F2 score:', fbeta_score(y_test, y_pred, 2))"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Results on test set...\n","[[56865    23]\n"," [   16    59]]\n","Classification accuracy: 0.9993153450485402\n","Sensitivity: 0.7866666666666666\n","Precision: 0.7195121951219512\n","F2 score: 0.7722513089005236\n"],"name":"stdout"}]}]}